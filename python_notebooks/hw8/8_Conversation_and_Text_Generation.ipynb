{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DCJfD0Iu4Tk"
   },
   "source": [
    "# Week 8 - Conversation and Text Generation\n",
    "Many natural language activities boil down to text generation, especially the back-and-forth nature of natural conversation and question answering. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "\n",
    "Much recent NLP research is on text generation. Before the phenomenal product ChatGPT (OpenAI) was released at the end of 2022,  this is the primary use of large language models like GPT-3/4 (OpenAI), Wu Dao (Beijing Academy of AI), and Gopher (DeepMind). Then everything went wild in 2023. We saw tons of open-sourced LLMs were released like the Llama series (Meta), and how others techs wanted to compete with OpenAI by releasing models like Claude (Anthropic), Bard (Google), Gemini (Google x2), etc. The success of these models have prompted debate over whether the risks and perils of artificial general intelligence (AGI) is approaching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAiiglFku4Tm",
    "outputId": "c0cffd6a-3a70-46a3-aad2-20e8f782d2d1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
      "  Cloning https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git to /tmp/pip-req-build-hngzuugu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git /tmp/pip-req-build-hngzuugu\n",
      "  Resolved https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git to commit b17a265d3b8253424e5b38872457f7437909a65d\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.25.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (2.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.5.3)\n",
      "Collecting python-docx (from lucem-illud==8.0.1)\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.6/239.6 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (9.4.0)\n",
      "Collecting pdfminer2 (from lucem-illud==8.0.1)\n",
      "  Downloading pdfminer2-20151206-py2.py3-none-any.whl (117 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.8/117.8 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting GitPython (from lucem-illud==8.0.1)\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m195.4/195.4 kB\u001B[0m \u001B[31m22.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.9.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.11.4)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.13.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.8.1)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (4.3.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.7.1)\n",
      "Collecting pyanno3 (from lucem-illud==8.0.1)\n",
      "  Downloading pyanno3-2.0.2.tar.gz (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.8/76.8 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (4.12.3)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.20.1)\n",
      "Collecting boto3 (from lucem-illud==8.0.1)\n",
      "  Downloading boto3-1.34.49-py3-none-any.whl (139 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m139.3/139.3 kB\u001B[0m \u001B[31m16.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.2.1)\n",
      "Collecting pydub (from lucem-illud==8.0.1)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting speechrecognition (from lucem-illud==8.0.1)\n",
      "  Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m32.8/32.8 MB\u001B[0m \u001B[31m31.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pysoundfile (from lucem-illud==8.0.1)\n",
      "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.19.3)\n",
      "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (7.34.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->lucem-illud==8.0.1) (2.5)\n",
      "Collecting botocore<1.35.0,>=1.34.49 (from boto3->lucem-illud==8.0.1)\n",
      "  Downloading botocore-1.34.49-py3-none-any.whl (12.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.0/12.0 MB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->lucem-illud==8.0.1)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->lucem-illud==8.0.1)\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.1/82.1 kB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->lucem-illud==8.0.1) (6.4.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython->lucem-illud==8.0.1)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (67.7.2)\n",
      "Collecting jedi>=0.16 (from IPython->lucem-illud==8.0.1)\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m48.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (3.0.43)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (2.16.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (2.8.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (4.66.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lucem-illud==8.0.1) (2023.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pdfminer2->lucem-illud==8.0.1) (1.16.0)\n",
      "Collecting traits (from pyanno3->lucem-illud==8.0.1)\n",
      "  Downloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m56.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from pysoundfile->lucem-illud==8.0.1) (1.16.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->lucem-illud==8.0.1) (4.9.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx->lucem-illud==8.0.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (2024.2.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (2.31.6)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (2024.2.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lucem-illud==8.0.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.1.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud==8.0.1) (2.21)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->lucem-illud==8.0.1)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->lucem-illud==8.0.1) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->lucem-illud==8.0.1) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->lucem-illud==8.0.1) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->lucem-illud==8.0.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->lucem-illud==8.0.1) (2.16.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->lucem-illud==8.0.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->lucem-illud==8.0.1) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->lucem-illud==8.0.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->lucem-illud==8.0.1) (2.1.5)\n",
      "Building wheels for collected packages: lucem-illud, pyanno3\n",
      "  Building wheel for lucem-illud (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for lucem-illud: filename=lucem_illud-8.0.1-py3-none-any.whl size=34989 sha256=1015503752c2321464fc729bd723d3f2fd000e3c08ac6a2882dc40f4a2d26805\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2h56jril/wheels/4c/84/7b/d89dec34fb910351cf618bf262d1c926f9f68779ef24028427\n",
      "  Building wheel for pyanno3 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyanno3: filename=pyanno3-2.0.2-py3-none-any.whl size=116965 sha256=41f2c2a503e4175891d40c35b4d88aef61667e73c7dae694211e07b2febbe104\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/0c/9f/21212c187c1edb1647c18e0a24b6b213b3bb7dee5aa94bfd72\n",
      "Successfully built lucem-illud pyanno3\n",
      "Installing collected packages: pydub, traits, smmap, python-docx, pdfminer2, jmespath, jedi, speechrecognition, pysoundfile, pyanno3, gitdb, botocore, s3transfer, GitPython, boto3, lucem-illud\n",
      "Successfully installed GitPython-3.1.42 boto3-1.34.49 botocore-1.34.49 gitdb-4.0.11 jedi-0.19.1 jmespath-1.0.1 lucem-illud-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.25.1 pysoundfile-0.9.0.post1 python-docx-1.1.0 s3transfer-0.10.0 smmap-5.0.1 speechrecognition-3.10.1 traits-6.4.3\n"
     ]
    }
   ],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "!pip install -U git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "import lucem_illud\n",
    "\n",
    "import sklearn #For generating some matrices\n",
    "import pandas as pd #For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #Makes the plots look nice\n",
    "import seaborn as sns\n",
    "import scipy #Some stats\n",
    "import nltk #a little language code\n",
    "from IPython.display import Image #for pics\n",
    "\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lk2R1QxOu4Tm"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1UsG9ZsWu4Tn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcxh_Xuzu4Tn"
   },
   "source": [
    "# ConvoKit\n",
    "As we alluded to in Week 7 with causal inference, [ConvoKit](https://convokit.cornell.edu/) is an exciting platform for conversational analysis developed by Jonathan Chang, Calem Chiam, and others, mostly at Cornell. Keep this in mind if you are interested in a final project with conversational data such as Twitter threads or movie scripts. They have an [interactive tutorial](https://colab.research.google.com/github/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/Introduction_to_ConvoKit.ipynb), which we include some examples from below. Most of the following text and code is authored by them.\n",
    "\n",
    "These ConvoKit corpora can be used for the next exercise in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpQTyQjBUonG",
    "outputId": "99f2f057-ac89-434b-be71-a54d60c8c880"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting convokit\n",
      "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m183.2/183.2 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n",
      "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
      "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
      "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
      "Collecting dill>=0.2.9 (from convokit)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
      "Collecting clean-text>=0.6.0 (from convokit)\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting unidecode>=1.1.1 (from convokit)\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m235.5/235.5 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.2)\n",
      "Collecting pymongo>=4.0 (from convokit)\n",
      "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m677.2/677.2 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
      "Collecting dnspython>=1.16.0 (from convokit)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m307.7/307.7 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m175.4/175.4 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.4/53.4 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.12.25)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.3.5->convokit) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n",
      "Building wheels for collected packages: convokit, emoji\n",
      "  Building wheel for convokit (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=045e4e97d912e574bd741dfba85fe5ee424084d87df5bee01778f04a302245b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
      "  Building wheel for emoji (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=c8e9c94bed6a0e800413f0d5f85d0d6914fae639f82c4420e9a50728a3b6f5df\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
      "Successfully built convokit emoji\n",
      "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
      "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.1 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.2 unidecode-1.3.8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import convokit\n",
    "except ModuleNotFoundError:\n",
    "    !pip install convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mMQXf1xHUonH"
   },
   "outputs": [],
   "source": [
    "# for pretty printing of cells within the Colab version of this notebook\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "RLHbOOAzUonI",
    "outputId": "723a620b-5dbd-4a78-d684-cfe120ca163c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "import convokit\n",
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5VIPx2oUonI"
   },
   "source": [
    "### Loading a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_lhin9LUonJ"
   },
   "source": [
    "A Corpus represents a conversational dataset. We typically begin our analysis by loading a Corpus. A list of existing datasets already in ConvoKit format can be found [here](https://convokit.cornell.edu/documentation/datasets.html).\n",
    "\n",
    "A growing list of many other conversational datasets covering a variety of conversational settings are available in ConvoKit, such as face-to-face (e.g. the [*Intelligence Squared Debates corpus*](https://convokit.cornell.edu/documentation/iq2.html)), institutional (e.g. the [*Supreme Court Oral Arguments corpus*](https://convokit.cornell.edu/documentation/supreme.html)), fictional (e.g. the [*Cornell Movie Dialog Corpus*](https://convokit.cornell.edu/documentation/movie.html)), or online  (e.g. all talkpage conversations on [*Wikipedia Talk Pages*](https://convokit.cornell.edu/documentation/wiki.html) and a full dump of [*Reddit*](https://convokit.cornell.edu/documentation/subreddit.html)).\n",
    "\n",
    "For this tutorial, we will primarily be using the *r/Cornell* subreddit corpus to demo various ConvoKit functionality, and occasionally the [*Switchboard Dialog Act Corpus*](https://convokit.cornell.edu/documentation/switchboard.html) (a collection of anonymized five-minute telephone conversations) as a contrasting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "488jR7pVUonJ",
    "outputId": "d5170b60-034f-4dc2-9c9e-315c8a301cb8"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading subreddit-Cornell to /root/.convokit/downloads/subreddit-Cornell\n",
      "Downloading subreddit-Cornell from http://zissou.infosci.cornell.edu/convokit/datasets/subreddit-corpus/corpus-zipped/CookingScrewups~-~CrappyDesign/Cornell.corpus.zip (11.2MB)... Done\n",
      "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
      "# Default Backend Parameters\n",
      "db_host: localhost:27017\n",
      "data_directory: ~/.convokit/saved-corpora\n",
      "default_backend: mem\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(download('subreddit-Cornell'))\n",
    "\n",
    "# You can try a different corpus if you want.\n",
    "#corpus = Corpus(download('diplomacy-corpus'))\n",
    "#corpus = Corpus(download('switchboard-corpus'))\n",
    "#corpus = Corpus(download('reddit-corpus-small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "rnBisd3QUonL",
    "outputId": "61dcd1b5-55d2-4723-c06a-75185da48732"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Speakers: 7568\n",
      "Number of Utterances: 74467\n",
      "Number of Conversations: 10744\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTcJWCgzUonL"
   },
   "source": [
    "### Corpus components: Conversations, Utterances, Speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnnTm_ALUonL"
   },
   "source": [
    "Every Corpus has three main components: [Conversations](https://convokit.cornell.edu/documentation/conversation.html), [Utterances](https://convokit.cornell.edu/documentation/utterance.html), and [Speakers](https://convokit.cornell.edu/documentation/speaker.html). Just as in real life, in ConvoKit, Conversations are some sequence of Utterances, where each Utterance is made by some Speaker. Let's look at an example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xW9eeQrcUonS",
    "outputId": "4251a768-ddd0-4414-b07e-60ab07be5808"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvoKitMeta({'title': 'Anyone knows when the Fall 2017 roster is coming out?', 'num_comments': 4, 'domain': 'self.Cornell', 'timestamp': 1487530863, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': None, 'stickied': False, 'author_flair_text': ''})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# This is a Reddit thread\n",
    "corpus.random_conversation().meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "TeR71mPgUonT",
    "outputId": "6c8f951a-0ef7-4e29-c19f-beb67848af77"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvoKitMeta({'score': 0, 'top_level_comment': 'c98v4qy', 'retrieved_on': 1431667172, 'gilded': 0, 'gildings': None, 'subreddit': 'Cornell', 'stickied': False, 'permalink': '', 'author_flair_text': ''})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# This is a Reddit post or comment.\n",
    "corpus.random_utterance().meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LnQcLKBxUonS",
    "outputId": "21851420-8a79-4fb1-87b1-b9c01766ba2f"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x7c93f986d4e0>, 'id': 'TaedW', 'meta': ConvoKitMeta({})})"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# The r/Cornell Corpus does not have speaker metadata.\n",
    "#corpus.random_speaker().meta\n",
    "\n",
    "#Speaker do have an 'id' which is their Reddit username, as seen here.\n",
    "corpus.random_speaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "kpfMX5_bu4Tp",
    "outputId": "5266dd2d-a786-4e30-ece9-e76f70bedeed"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I was just reading about the Princeton Mic-Check and it's getting [national press](http://www.bloomberg.com/news/2011-12-29/princeton-brews-trouble-for-us-1-percenters-commentary-by-michael-lewis.html).\n",
      "\n",
      "I want to get a sense of what people felt like around campus. Anything interesting happen? Anything interesting coming up?\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through these objects as we iterate lists or DataFrames in Python.\n",
    "for utt in corpus.iter_utterances():\n",
    "    print(utt.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP83knAGu4Tp"
   },
   "source": [
    "Conversations, Utterances, and Speakers are each interesting, but the magic of conversational analysis is connecting them. For example, we can get all the Conversations in which a Speaker has participated and all the Utterances they have made. To make it more interesting, we can find a Speaker to study by navigating from a random Utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "fAyGVo-Ou4Tp",
    "outputId": "b6bd1e00-3ae2-430a-d913-381adfaa5584"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7c93f986d4e0>, 'id': 'o0145', 'meta': ConvoKitMeta({'title': 'course schedule planner for Cornell University', 'num_comments': 7, 'domain': 'self.Cornell', 'timestamp': 1325530635, 'subreddit': 'Cornell', 'gilded': -1, 'gildings': None, 'stickied': False, 'author_flair_text': ''})}),\n",
       " Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7c93f986d4e0>, 'id': 'o1gca', 'meta': ConvoKitMeta({'title': 'should we advertise ourselves on facebook?', 'num_comments': 9, 'domain': 'self.Cornell', 'timestamp': 1325620506, 'subreddit': 'Cornell', 'gilded': -1, 'gildings': None, 'stickied': False, 'author_flair_text': ''})}),\n",
       " Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7c93f986d4e0>, 'id': 'o0ss4', 'meta': ConvoKitMeta({'title': 'oh look.. a picture!', 'num_comments': 45, 'domain': 'self.Cornell', 'timestamp': 1325571377, 'subreddit': 'Cornell', 'gilded': -1, 'gildings': None, 'stickied': False, 'author_flair_text': ''})})]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# consider this sequence of operations that highlight how to navigate between components\n",
    "utt = corpus.random_utterance()\n",
    "convo = utt.get_conversation() # get the Conversation the Utterance belongs to\n",
    "spkr = utt.speaker # get the Speaker who made the Utterance\n",
    "\n",
    "spkr_convos = list(spkr.iter_conversations())\n",
    "\n",
    "# Display up to 3 of their conversations.\n",
    "spkr_convos[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHkHRCiAu4Tp"
   },
   "source": [
    "For a more qualitative feel of the data, you can display a Conversation. For Reddit data, this is a single thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "gCVGO_rZu4Tp",
    "outputId": "abfdaf75-c56b-4875-b5ec-5e7f55940ccb"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "    i have no idea how to respond other than some dank memes.\n",
      "\n",
      "Ayyyy lmao.\n",
      "\n",
      "    ( ͡° ͜ʖ ͡°)\n",
      "\n",
      "    That sex column always shows up on my news reader. \n",
      "\n",
      "Arguably some of the most i\n",
      "\n",
      "    [deleted]\n",
      "\n",
      "        That much is obvious...\n",
      "\n",
      "    That column is in really poor taste. Have some self-respect Sun!\n"
     ]
    }
   ],
   "source": [
    "# We truncate sentences at character 80 to avoid making this notebook too long!\n",
    "convo.print_conversation_structure(lambda utt: utt.text[:80] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJloyysmu4Tp"
   },
   "source": [
    "There is a lot more to ConvoKit that we encourage you to explore, especially their [tutorial](https://colab.research.google.com/github/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/Introduction_to_ConvoKit.ipynb), but the ability to seamlessly navigate between the Utterances, Conversations, and Speakers of a Corpus is extremely valuable for social science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB8Il5Fxu4Tp"
   },
   "source": [
    "## <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that use ConvoKit to analyze a Corpus other than 'subreddit-Cornell', including at least one function you find in the package not used above. You can also generate a ConvoKit Corpus from your own dataset based on [their Corpus from .txt files tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/converting_movie_corpus.ipynb) or [their Corpus from pandas tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/corpus_from_pandas.ipynb), but that may be time-consuming for a weekly assignment. (It could be a great idea for your final project!)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I am using 'Winning Arguments (ChangeMyView)' Corpus, and I am going to see how many users win an arguemnts. The new function I am using is `get_utterance`."
   ],
   "metadata": {
    "id": "mbtdLvMU_OjO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "corpus = Corpus(filename=download(\"winning-args-corpus\"))\n",
    "corpus.print_summary_stats()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7bd4pRAS-qOz",
    "outputId": "b659f9ef-3df5-4e49-edbf-f7171e539df4"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading winning-args-corpus to /root/.convokit/downloads/winning-args-corpus\n",
      "Downloading winning-args-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/winning-args-corpus/winning-args-corpus.zip (73.7MB)... Done\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cnhre1n has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cnhs1jf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cn7mmnt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cn66mck has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmt6w97 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmsgxzm has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmsitjr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmqqd49 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmqsp80 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmr57l3 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cmihu76 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cm85qp1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cm6tktt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_clz6jxt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cm09icp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cly0oho has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cly8wzq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_clxpe30 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_clv6oas has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cls8zr2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_clj3tcp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_clhtmr0 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cler134 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_claiask has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl96qxj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl92hgd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl92jdc has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl7lil7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_claaeq7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl6xkrl has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl6yu6h has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl6ywwj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl6yxf2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl6u197 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl65sus has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl67wux has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl5n0e5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl55jtf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl53psw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl54hde has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl5490t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl38tmp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cl2rq86 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ckujlyq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ckujdib has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cklsm40 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ckmghf2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cknmezc has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ckj2g0i has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ck8j9e4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ck8cwb6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjxlz5l has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjntq7s has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjnp5hy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjiqn1e has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjh7hr0 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjh6yab has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cjcd62j has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj8l1kv has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj8zjae has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj7yiy4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj7o50t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj50shk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cizh6xb has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cj0ukb7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ciz9aka has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cize32v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_civr4mn has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cip166t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cil3zav has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cijy9lk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cijcups has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cifu4zp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cid30or has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ci8wo4e has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ci9fbtw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ci2pixf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ci2pcku has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ci2cc99 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chz0gqi has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chw609b has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chv6j3s has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chv26as has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_churmo0 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chswmlq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chsxa75 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chnlrqw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chl8bsy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chiw401 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chihwy1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chh0sby has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chbrqn7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch7rh7p has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch6watf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch6ssui has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch61v2p has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch3ng8c has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgrm70r has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgrz99m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgsegx6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgnj6j9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ch073vt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgkn5ae has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgiihu5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgieyuu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgig23g has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgie34t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgh4ifl has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cggr63w has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cggd3v7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cggutk5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgeiql6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgej6dy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cge3th4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cgedp3m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cg8kgbj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfwc45l has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfv3enu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfpx424 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfl78p1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfilsz7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfj8wvg has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cffznn5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cffo8l7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfdq2z7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfc9c4b has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfcrvtf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfbxgor has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfb4q61 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cfb5l7x has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cf99z8w has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cf2h04o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ceptv5v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ceo2hjj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ceorou4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cenyp1x has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cemye2y has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_celsimn has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ceehosv has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cebrznc has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ceconce has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cebnxzk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cebd6e3 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cec3nwt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_chv7nyk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cedktpa has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ce9dob3 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ce5q8tj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ce4gt8u has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ce0p904 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cdosehh has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cdiarjx has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cda83lr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cd759lq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cd4pzb7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cd4p7s1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cd4zoy7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cdn4qk9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccue3mg has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccphf6t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccphsld has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccmc12f has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccjdtdf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cciu2c8 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccizwjs has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccivs3y has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccj0iqr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cciupg2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cch8ibq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccgulma has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccgulwv has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccgnj8i has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cch2tad has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cch9i4l has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cch9wwe has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccgvmn6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cce35j0 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ccdamor has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cce8w7z has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cce42qt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cc9x38t has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cc302nb has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbyfuze has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbvq8lh has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbrec2x has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbrmg6l has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbq8ij7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbp1a4b has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cboztg2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbou2b5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbpau3m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbokj1o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cboz1er has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbmr775 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cblsags has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cbqpqwi has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb9lo66 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb9lrsq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb9zx65 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb8beut has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb7zk0m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb6c4sq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb33ath has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb1kfhi has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb0ahbi has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cb0bzr4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cawc0b9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cawjc4y has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caxt2vf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caukmyp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caurnnc has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cavlgpq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caulblj has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cauridr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_casyucd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_catl342 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_camzwda has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_camssul has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_can6u3l has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_calik3u has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caljueg has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cajb0py has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cajbvqf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caajlk6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_caachmu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cabc5l7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9xkwd1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9za8qk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9x7xos has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9zr42i has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9tlmhq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9r6a05 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ca0tpgp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9r7py2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9rnobm has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9rp0so has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9qubp4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9qwaar has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9pfs2o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9lc68q has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9jlurv has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9hall9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9gkb83 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c9bvjlq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c98s9ip has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c98o57y has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c99ahf3 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c990rx6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c97eoi9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c97acob has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c97ah5c has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c95q7s3 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c95kdch has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_c95l9ml has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqzqi7q has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqzmedm has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cr13cnd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqzr1lp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqyom18 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqybmxe has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqy7ahu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqybfct has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqyuen2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqy83y1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqxoxi5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqn9nyq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqnhdk2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqn3mch has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqn7pac has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqmw3di has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqf8ryl has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqf8mpf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqfb5xa has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqfhjkq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqgarpy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqdccrd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqctlqr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqcthds has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqkelp2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqd8j90 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqd8a8j has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cqcrdrw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq8xz69 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq97x51 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq8zb08 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq959on has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq9ci0g has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cq97u6v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cptu8ww has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpstv77 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpsxhki has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpswzfn has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpuguvq has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpvfq3o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cptriew has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpsxudr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpszfeh has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpqft77 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpqfs0u has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cprk62o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpnulph has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpnn77o has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpnju02 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpn5p75 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpni19v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpo4x6m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpbijg5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cpbfv95 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coz6meu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coz24er has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coz3omg has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_copv0tz has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cor1wk2 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coq0wpg has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coqe7nl has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cokywqo has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cokwcae has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cokx8ik has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cokyzdf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_col2sjk has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cokxscn has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cojw16m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coezsna has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cof1lb9 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cohpfyt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cof4o1h has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coeyzd4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coai23q has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coa8l6z has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coa80dz has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_coa8l84 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co68zqm has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co6a4yb has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co6ef8m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co6ba44 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co6rpqr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co69x71 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_co6a13x has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cnq3mwr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cnq4ycs has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cnqs4bp has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cunpy0j has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cuior6e has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cufvp2u has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu7mdqo has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu7xcz4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu5cv9a has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu3h1wa has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu272pa has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cu0qopb has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctzro35 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctxzpb4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctwh70b has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctqswb6 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctqjqcd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctoncx7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctq5xed has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctjjz2v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cti68mr has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctiqywc has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cthu9hx has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctdqpne has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctdkusf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctdhx1w has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctcfj2v has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct5ul8m has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct5m7v4 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct4klhi has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct3sgfd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct5bruu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct1rmby has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cstl2de has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cstboz7 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cstbdw5 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ct7dtjh has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csk5kbs has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctam9yz has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csh3dqw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csislrs has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csed3sw has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctalrap has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csernqu has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_csdn1ui has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cs8mmtf has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cs4re28 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_ctb2nf1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cs1tl9q has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_crul067 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_crusrpd has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_crjkijn has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cre72f1 has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_creewvt has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_creh7rb has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_crctrfy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_crbkihy has been casted to a string.\n",
      "\u001B[91mWARNING: \u001B[0mUtterance text must be a string: text of utterance with ID: t1_cra0ei4 has been casted to a string.\n",
      "Number of Speakers: 34911\n",
      "Number of Utterances: 293297\n",
      "Number of Conversations: 3051\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "corpus.random_conversation().meta"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "IG3BNlXe-s6g",
    "outputId": "f1cb4177-9c42-4642-fb8d-ba4340834b74"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvoKitMeta({'op-userID': 'JohnnyForeskin', 'op-text-body': '**TIL how sexist the US dress code policies are.**\\n\\n**There was a bit of a misunderstanding here. I was under the assumption that the dress codes were equal (for the most part, they are where I\\'m from) and didn\\'t shit on women in the way that they do. I guess it\\'s tough to argue a point that is dependent on geography.**\\n\\n**I still think as long as the dress code is consistent between men and women, it\\'s fine to be strict with it. I do not agree with the dress codes that are unequal in this aspect.**\\n\\n\\n\\n\\n\\nHere she goes:\\n\\nGirls are not sexual objects, but it is in a man\\'s nature to be attracted to the opposite gender in a sexual way. For most males, the more revealing a woman is of her body, the more sexual stirrings they begin to experience.\\n\\n Let\\'s say the teacher is a male and a select few females, who are 16 years old (age of consent where I\\'m from), are wearing very skimpy clothing. This does not make them sluts or whores, or anything like that, but it does distract the male teacher *because it is in his nature* to be this way.\\n\\nNow all the teacher can think about are these girls and can\\'t keep his mind on the task at hand. The class suffers due to this and it is a lose-lose situation as the teacher cannot do his job. It is not necessarily the females\\' fault here, but it isn\\'t completely the male\\'s fault either.\\n\\nIf the schools require females and males (although, females have different triggers) to wear decent clothing that smother at least part of this urge, then no one suffers because of it.\\n\\nBy having a strict dress code, you eliminate the risk of having a \"situation\" between a male and female when the main goal is to educate, not reproduce.\\n\\nMale\\'s are genetically wired to spread our seed as much as possible. Sure, they are expected and required to control the execution of this urge when the other party is not interested, but the mere fact that the urge exists is enough to distract them from their goals.\\n\\nYes, the school is taking away the freedom of student\\'s choice. But, the teachers aren\\'t allowed to go up to the student and ask them out **ASSUMING HERE THAT THEY ARE OF AGE** because they must remain \"professional.\" Why aren\\'t the students required to be \"professional\" and dress \"appropriately?\"\\n_____\\n\\n&gt; *Hello, users of CMV! This is a footnote from your moderators. We\\'d just like to remind you of a couple of things. Firstly, please remember to* ***[read through our rules](http://www.reddit.com/r/changemyview/wiki/rules)***. *If you see a comment that has broken one, it is more effective to report it than downvote it. Speaking of which,* ***[downvotes don\\'t change views](http://www.reddit.com/r/changemyview/wiki/guidelines#wiki_upvoting.2Fdownvoting)****! If you are thinking about submitting a CMV yourself, please have a look through our* ***[popular topics wiki](http://www.reddit.com/r/changemyview/wiki/populartopics)*** *first. Any questions or concerns? Feel free to* ***[message us](http://www.reddit.com/message/compose?to=/r/changemyview)***. *Happy CMVing!*', 'op-title': 'CMV: High school dress code should remain as strict as it is.', 'pair_ids': ['p_448', 'p_447'], 'train': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "utterance_ids = corpus.get_utterance_ids()\n",
    "\n",
    "u = 0\n",
    "s = 0\n",
    "for each in utterance_ids:\n",
    "    if corpus.get_utterance(each).meta[\"success\"] == 0: # this get_utterance is new\n",
    "        u = u + 1\n",
    "    if corpus.get_utterance(each).meta[\"success\"] == 1:\n",
    "        s = s + 1\n",
    "\n",
    "print(\"number of successful comments: \" + str(s))\n",
    "print(\"number of unsuccessful comments: \" + str(u))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ryzb0q__AwhO",
    "outputId": "3ec73beb-8a1c-405d-de69-37315a4e411b"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of successful comments: 12420\n",
      "number of unsuccessful comments: 7294\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Account9726\n",
    "corpus.get_speaker('Account9726') # get conversation of a random speaker"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ZqIjhRjOIAR5",
    "outputId": "9472f857-509a-49d6-d795-6838beab83cb"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x7c93f9801c90>, 'id': 'Account9726', 'meta': ConvoKitMeta({})})"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTLm96n5u4Tp"
   },
   "source": [
    "## Creating networks of agents from corpora\n",
    "\n",
    "Now let's return to the Davies corpora (specifically, Soap Operas) to see how we can extract actors and build a network of their relationships in the texts.\n",
    "\n",
    "We'll use the `lucem_illud.loadDavies()` function to get the dataframe. Make sure to download `SOAP.zip` from DropBox, unzip, and edit the following line with the path to that file. This code may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUb1szm43BQV",
    "outputId": "ea0a7b79-4505-43bc-8b96-3ece2a71dd2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "wa5SHZElu4Tq",
    "outputId": "06fcf27a-b2f1-44e3-e14f-1381ba5c9a0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpora_address = \"/content/drive/MyDrive/Colab Notebooks/Computational Content Analysis/Homework-Notebooks-2024-Winter/data/SOAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gAsNS0tu4Tq"
   },
   "outputs": [],
   "source": [
    "soap_texts = lucem_illud.loadDavies(corpora_address, num_files=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLX8EyJ4u4Tq"
   },
   "source": [
    "We now use the source to see how the data is stored. Note that this is different from the movies corpus, and that we will need to use a different aggregating method to store the data. Each dataset would have a different approach, but they are all very similar, it depends on how the data is stored. Here multiple textids match multiple scripts, so our soap dataframe would be structured a little differently.\n",
    "\n",
    "You can see the first 20 lines of the source file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMK-JFuXu4Tq"
   },
   "outputs": [],
   "source": [
    "zfile = zipfile.ZipFile(corpora_address + \"/soap_sources.zip\")\n",
    "source = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMj8Rj8Ou4Tq"
   },
   "outputs": [],
   "source": [
    "for file in zfile.namelist():\n",
    "    with zfile.open(file) as f:\n",
    "        for line in f:\n",
    "            source.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WuMMQBQu4Tq"
   },
   "outputs": [],
   "source": [
    "source[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot2kZRB8u4Tq"
   },
   "outputs": [],
   "source": [
    "soap_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3RxxObGu4Tr"
   },
   "outputs": [],
   "source": [
    "for soap in source[3:]:\n",
    "    try:\n",
    "        textID, year, show, url = soap.decode(\"utf-8\").split(\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    if show.strip() not in soap_dict:\n",
    "        soap_dict[show.strip()] = []\n",
    "    if show.strip() in soap_dict:\n",
    "        try:\n",
    "            soap_dict[show.strip()].append(soap_texts[textID.strip()])\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll0kJ-aru4Tr"
   },
   "outputs": [],
   "source": [
    "soap_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVGLhkScu4Tr"
   },
   "outputs": [],
   "source": [
    "soap_df = pd.DataFrame(columns=[\"Soap Name\", \"Tokenized Texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2WG3rpPu4Tr"
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2SlNnTXu4Ts"
   },
   "outputs": [],
   "source": [
    "for soap in soap_dict:\n",
    "    # since there were multiple lists\n",
    "    print(soap)\n",
    "    full_script = []\n",
    "    for part in soap_dict[soap]:\n",
    "        full_script = full_script + part\n",
    "    soap_df.loc[i] = [soap, full_script]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDHoEjZ3u4Ts"
   },
   "outputs": [],
   "source": [
    "soap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed-L10sgu4Ts"
   },
   "source": [
    "We now have each Soap, and each of the Tokenized texts. Let us see what kind of information we can get. These are American soap operas, and are likely to be cheesy and dramatic (an understatment). A fun start would be to make networks of each of the actors and actresses in these soaps.\n",
    "\n",
    "What would be a good way to create a network? Maybe everytime someone talks to someone we add one weight? But we wouldn't want to add weights whenever it's a different scene - or maybe we do? Let us look at the text and figure it out.\n",
    "\n",
    "Note that we didn't add the year here because it spans over multiple years. If we are doing different kinds of analysis we would want to a years column as well.\n",
    "\n",
    "In my dataframe, Days of Our Lives is the 4th corpora, and I conducted my basic analysis on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqbNeFcFu4Ts"
   },
   "outputs": [],
   "source": [
    "dool = soap_df['Tokenized Texts'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc0GN_A8u4Ts"
   },
   "outputs": [],
   "source": [
    "' '.join(dool[0:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxRfbaNNu4Tt"
   },
   "source": [
    "Hmmm... we can't do our normal text processing. But this provides us with an interesting oppurtunity: every '@!' is followed by some useeful information. Let us do a quick check of how many characters exist here, and how many times they speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLMOTu8vu4Tt"
   },
   "outputs": [],
   "source": [
    "characters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRDRSj4iu4Tt"
   },
   "outputs": [],
   "source": [
    "for token in dool:\n",
    "    if token[0] == '@':\n",
    "        # all characters or actions start with @, so we add that to character\n",
    "        if token[2:] not in characters:\n",
    "            characters[token[2:]] = 0\n",
    "        if token[2:] in characters:\n",
    "            characters[token[2:]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTPwcelKu4Tt"
   },
   "outputs": [],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMgc_hE2u4Tt"
   },
   "source": [
    "Wow, that's a lot of characters: but we notice a '@!' between certain actions too, such as screaming and sobbing. Let us maybe only look for characters with a high number of appearances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKtIeqYRu4Tu"
   },
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    if characters[character] > 2000:\n",
    "        print(character, characters[character])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blpP9ybfu4Tu"
   },
   "source": [
    "Let's check these folks out on the interwebz...a image of search of the name + \"days of our lives\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EcmtStiu4Tu"
   },
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_john.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNL8C-78u4Tu"
   },
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_brady.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "064MPyDFu4Tu"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_hope.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7JYjj1Zu4Tu"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_philip.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xwnbjo9Cu4Tu"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_marlena.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxO2hpxHu4Tu"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_kate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w35A6ltFu4Tu"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_bo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydQGsGvpu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_chloe.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP_ZcDwTu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_sami.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UH8aDZHu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_shawn.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i95yDU-Hu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_belle.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxOD67Imu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_lucas.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC-4NzJxu4Tv"
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_nicole.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te4bqgIXu4Tv"
   },
   "source": [
    "These are definitely big, long-time players in the dramatic Days narrative. It would make sense to create a graph where each character who appears over 2000 times is a node, and each time they talk to each other, we add one to their weight. We should also store all the things these chracters say: that's useful information.\n",
    "\n",
    "So we now iterate through the tokens in a manner where we can capture this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w53JQIXou4Tv"
   },
   "outputs": [],
   "source": [
    "actor_network = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfBhD5bDu4Tv"
   },
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    if characters[character] > 2000:\n",
    "        actor_network.add_node(character, lines_spoken= characters[character], words=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86VTpmOou4Tv"
   },
   "outputs": [],
   "source": [
    "len(actor_network.nodes.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t02UF8ZFu4Tv"
   },
   "outputs": [],
   "source": [
    "actor_network.nodes.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2Ka980gu4Tv"
   },
   "outputs": [],
   "source": [
    "actor_network.nodes['Sami']['lines_spoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7sqeWfBu4Tv"
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue8KIOQxu4Tv"
   },
   "source": [
    "The following lines of code creates the graph of actors and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_bBu0L_u4Tw"
   },
   "outputs": [],
   "source": [
    "for token in dool:\n",
    "    i += 1\n",
    "    if i > len(dool):\n",
    "        break\n",
    "    if token[0] == \"@\":\n",
    "        if token[2:] in actor_network.nodes():\n",
    "            j = i\n",
    "            for token_ in dool[i:]:\n",
    "                if token_[0] == \"@\":\n",
    "                    # if both the characters exist in the graph, add a weight\n",
    "                    if token_[2:] != token[2:] and token_[2:] in actor_network.nodes():\n",
    "                        if (token[2:], token_[2:]) not in actor_network.edges():\n",
    "                            actor_network.add_edge(token[2:], token_[2:], weight=0)\n",
    "                        if (token[2:], token_[2:]) in actor_network.edges():\n",
    "                            actor_network.edges[(token[2:], token_[2:])]['weight'] += 1\n",
    "                    break\n",
    "                j += 1\n",
    "            # adding characters sentences\n",
    "            actor_network.nodes[token[2:]]['words'].append(dool[i:j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ihe8wrPlu4Tw"
   },
   "outputs": [],
   "source": [
    "nx.draw(actor_network, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgceZkgru4Tw"
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for node in actor_network.nodes():\n",
    "    l = []\n",
    "    for node_ in actor_network.nodes():\n",
    "        if node == node_:\n",
    "            l.append(0)\n",
    "        else:\n",
    "            l.append(actor_network.edges[(node, node_)]['weight'])\n",
    "    L.append(l)\n",
    "M_ = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pd.DataFrame(M_, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxat6VEou4Tw"
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "c = list(greedy_modularity_communities(actor_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BaszQk7u4Tw"
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oysBkvRgu4Tw"
   },
   "source": [
    "### Finding structure in networks\n",
    "\n",
    "We now have a lot of useful information: we have a graph of all the characters, with their relationships with other characters, as well as all the words they've said. We tried finding communities, but it seems like everyone is connected to everyone: each of them form their own 'community'. Seems like people talk to each other a bunch in soaps.\n",
    "\n",
    "This is however, not the best network to find any meaningful patterns, as we can see with everyone connected to everyone. But as we can see with our heatmap, not everyone talks to everyone an equal amount. How about we only keep our \"important\" ties, where people are talking to each other a lot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reGmOgWtu4Tw"
   },
   "outputs": [],
   "source": [
    "smaller_actor_network = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neIvyeZlu4Tw"
   },
   "outputs": [],
   "source": [
    "for actor_1 in actor_network.nodes():\n",
    "    smaller_actor_network.add_node(actor_1, lines_spoken= actor_network.nodes[actor_1]['lines_spoken'], words=actor_network.nodes[actor_1]['words'])\n",
    "    for actor_2 in actor_network.nodes():\n",
    "        if actor_2!=actor_1 and actor_network.edges[(actor_1, actor_2)]['weight'] > 250:\n",
    "            smaller_actor_network.add_edge(actor_1, actor_2, weight=actor_network.edges[(actor_1, actor_2)]['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rn0OHDdnu4Tw"
   },
   "outputs": [],
   "source": [
    "nx.draw(smaller_actor_network, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVgfKUi7u4Tw"
   },
   "source": [
    "This is a lot more interesting: while the sets of characters overlap, there is still two distinct communities if you look at characters who regularly talk to each other!\n",
    "\n",
    "Let us see what our centrality measures look like, as well as communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMMB6zVnu4Tw"
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "c = list(greedy_modularity_communities(smaller_actor_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn4PJkmTu4Tw"
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY_7JTpIu4Tw"
   },
   "outputs": [],
   "source": [
    "dcentralities = nx.degree_centrality(smaller_actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-ERLcACu4Tw"
   },
   "outputs": [],
   "source": [
    "dcentralities['John'], dcentralities['Philip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me3dpUwnu4Tw"
   },
   "source": [
    "Our two different communities show up as detected by the networkx algorithm, and when we look at centralities, we can see that John is a lot more central than Philip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5roxG4Wu4Tx"
   },
   "source": [
    "Let us go back to our original graph, and see if the weight or number of similar appearences matches the text... how do we do this? Well, we already have the graph, and we also have information of who spoke to who. So we have our framework!\n",
    "\n",
    "This means we can explore ideas contained in two of the papers you will be reading: . “No country for old members: User lifecycle and linguistic change in online communities.”, and  “Fitting In or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness”, both of which you can access on Canvas.\n",
    "\n",
    "Let us use a simplified version of the papers, and check if a higher number of conversations might lead to a higher similarity between the word distributions for two characters. We can use the same divergences we used in the last notebook. Do you think it will match with the number of times each character was associated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCXANeVFu4Tx"
   },
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pd.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pd.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7puMo1jpu4Tx"
   },
   "outputs": [],
   "source": [
    "corpora = []\n",
    "for character in actor_network.nodes():\n",
    "    character_words = []\n",
    "    for sentence in actor_network.nodes[character]['words']:\n",
    "        for word in sentence:\n",
    "            character_words.append(word)\n",
    "    corpora.append(lucem_illud.normalizeTokens(character_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm_UV6F-u4Tx"
   },
   "outputs": [],
   "source": [
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I3zO1aru4Tx"
   },
   "outputs": [],
   "source": [
    "for p in corpora:\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference='KS'))\n",
    "    L.append(l)\n",
    "M = np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSxqGKImu4Tx"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "div = pd.DataFrame(M, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWl023jZu4Tx"
   },
   "outputs": [],
   "source": [
    "# np.corrcoef(M_, M)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xyq3Xc5u4Tx"
   },
   "source": [
    "With our two heatplots, we can attempt to do some rudimentary analysis. We can see from our previous plot that Shawn and Belle talk to each other a lot, so do Hope and Bo, and Nicole and Brady, and Lucas and Sami. Do they also talk *like* each other?\n",
    "\n",
    "Kind of, actually: all four of these pairs have a lower distance between them. Now I don't know anything about this particular soap... are these four pairs related? Are they in a relationship, either married or dating, or are they just really good friends?\n",
    "\n",
    "This lays out the frameworks which you can now use to explore your own networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKO923c-u4Tx"
   },
   "source": [
    "# Interactional influence\n",
    "\n",
    "Before we utilize transformers, let's see how to estimate the influence of one speaker on another in order to estimate a kind of interpersonal influence network based on a recent paper by Fangjian Guo, Charles Blundell, Hanna Wallach, and Katherine Heller entitled [\"The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation\"](https://arxiv.org/pdf/1411.2674.pdf). This relies on a kind of point process called a Hawkes process that estimate the influence of one point on another. Specifically, what they estimate is the degree to which one actor to an interpersonal interaction engaged in \"accomodation\" behaviors relative to the other, generating a directed edge from the one to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc7Y9uaAu4Tx"
   },
   "source": [
    "### First let's look at the output of their analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gR0IMwU0u4Tx"
   },
   "outputs": [],
   "source": [
    "example_name = '12-angry-men'   #example datasets: \"12-angry-men\" or \"USpresident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QISN5ARu4Tx"
   },
   "outputs": [],
   "source": [
    "result_path = '../data/Bayesian-echo/results/{}/'.format(example_name)\n",
    "if not os.path.isdir(result_path):\n",
    "    raise ValueError('Invalid example selected, only \"12-angry-men\" or \"USpresident\" are avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pEw_U9Cu4Tx"
   },
   "outputs": [],
   "source": [
    "df_meta_info = pd.read_table(result_path + 'meta-info.txt',header=None)\n",
    "df_log_prob = pd.read_csv(result_path + \"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n",
    "df_influence = pd.read_csv(result_path + 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n",
    "df_participants = pd.read_csv(result_path + 'cast.txt', delim_whitespace=True)\n",
    "person_id = pd.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n",
    "print()\n",
    "print ('Person : ID')\n",
    "person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhkiQmLou4Tx"
   },
   "outputs": [],
   "source": [
    "def getDensity(df):\n",
    "    data = df#_log_prob['log.prior']\n",
    "    density = scipy.stats.gaussian_kde(data)\n",
    "    width = np.max(data) - np.min(data)\n",
    "    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    return xs, density(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yHCXTcZu4Ty"
   },
   "source": [
    "### Plot MCMC (Markov Monte Carlo) trace and the density of log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl61ysK9u4Ty",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.plot(df_log_prob['log.prior'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Trace of log.prior')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "x,y = getDensity(df_log_prob['log.prior'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prior')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.plot(df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.likelihood')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "x,y = getDensity(df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.plot(df_log_prob['log.likelihood.test.set'])\n",
    "plt.title('Trace of log.likelihood.test.set')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood.test.set')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.prob')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtvCkOatu4Ty"
   },
   "source": [
    "### Plot the influence matrix between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuBGtsRPu4Ty"
   },
   "outputs": [],
   "source": [
    "A = int(np.sqrt(len(df_influence.columns))) #number of participants\n",
    "id_person = {}\n",
    "for p in person_id:\n",
    "    id_person[person_id[p]]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mKp2go6u4Ty"
   },
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pd.DataFrame(influence_matrix)\n",
    "\n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXt6Jjau4Ty"
   },
   "outputs": [],
   "source": [
    "stacked = df_influence.mean(axis=0)\n",
    "df_mean = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.std(axis=0)\n",
    "df_std = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZiDBL_Tu4Ty"
   },
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UxyVIo_u4Ty"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('MEAN of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuPFyfa5u4Ty"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('SD of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBEDpRYiu4Ty"
   },
   "source": [
    "### Barplot of total influences sent/received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_ncnkFuu4Tz"
   },
   "outputs": [],
   "source": [
    "sender_std = {} #sd of total influence sent\n",
    "reciever_std = {} #sd of total influence recieved\n",
    "for i in range(A):\n",
    "    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n",
    "    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n",
    "\n",
    "sent = df_mean.sum(axis=1) #mean of total influence sent\n",
    "recieved =df_mean.sum(axis=0) #mean of total influence recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcwtbGkru4Tz"
   },
   "source": [
    "Total influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "no35DLoeu4Tz"
   },
   "outputs": [],
   "source": [
    "print (\"\\t\\tTotal linguistic influence sent/received \")\n",
    "ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n",
    "\n",
    "plt.grid()\n",
    "wd=0.45\n",
    "ii=0\n",
    "for p in sender_std:\n",
    "    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    ii+=1\n",
    "ii=0\n",
    "for p in reciever_std:\n",
    "    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    ii+=1\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n",
    "plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('speaker',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erlt5NtCu4Tz"
   },
   "source": [
    "## Visualize Influence Network!\n",
    "\n",
    "You can visualize any of the influence matrices above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfvNdgTZu4Tz"
   },
   "source": [
    "Using networkx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdCdG8equ4Tz"
   },
   "outputs": [],
   "source": [
    "def drawNetwork(df,title):\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    G = nx.DiGraph()\n",
    "    for from_ in df.index:\n",
    "        for to_ in df.columns:\n",
    "            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n",
    "\n",
    "    pos = nx.spring_layout(G,k=0.55,iterations=20)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    weights = np.array(weights)\n",
    "    #weights = weights*weights\n",
    "    weights = 6*weights/np.max(weights)\n",
    "    print(title)\n",
    "\n",
    "    edge_colors=20*(weights/np.max(weights))\n",
    "    edge_colors = edge_colors.astype(int)\n",
    "#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
    "#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
    "#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
    "    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n",
    "            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n",
    "            alpha=0.75,arrows=True,arrowsize=20)\n",
    "    return edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZDfo4M0u4Tz"
   },
   "outputs": [],
   "source": [
    "# get quantile influence matrices for 25%, 50%, 75% quantile\n",
    "stacked = df_influence.quantile(0.25)\n",
    "df_q25 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.5)\n",
    "df_q50 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.75)\n",
    "df_q75 = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DepRYxPvu4Tz"
   },
   "outputs": [],
   "source": [
    "G_mean = drawNetwork(df_mean,'Mean Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVYazdidu4Tz"
   },
   "outputs": [],
   "source": [
    "G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHTB7ssLu4Tz"
   },
   "outputs": [],
   "source": [
    "G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByG73MOTu4Tz"
   },
   "outputs": [],
   "source": [
    "def fakeEnglish(length):\n",
    "    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n",
    "    return ''.join(np.random.choice(listd,length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWjnJUsWu4Tz"
   },
   "source": [
    "Your own dataset should contains 4 columns (with the same column names) as the artificial one below:\n",
    "\n",
    "- name: name of the participant\n",
    "- tokens: a list of tokens in one utterance\n",
    "- start: starting time of utterance (unit doesn't matter, can be 'seconds','minutes','hours'...)\n",
    "- end: ending time of utterance (same unit as start)\n",
    "\n",
    "There is no need to sort data for the moment.\n",
    "\n",
    "Below, we generate a fake collection of data from \"Obama\", \"Trump\", \"Clinton\"...and other recent presidents. You can either create your own simulation OR (better), add real interactional data from a online chat forum, comment chain, or transcribed from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gobXyvZMu4Tz"
   },
   "outputs": [],
   "source": [
    "script= []\n",
    "language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n",
    "role = 'Adult' #parameter, no need to tune\n",
    "\n",
    "for i in range(290):\n",
    "    dt = []\n",
    "    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n",
    "    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n",
    "    dt.append(faketokens) #fake utterance\n",
    "    dt.append(i*2+np.random.random()) # start time\n",
    "    dt.append(i*2+1+np.random.random()) # end time\n",
    "    script.append(dt)\n",
    "\n",
    "df_transcript = pd.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n",
    "df_transcript[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se3wxvNuu4Tz"
   },
   "source": [
    "Transform data into TalkbankXML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfBLvnEnu4Tz"
   },
   "outputs": [],
   "source": [
    "output_fname = 'USpresident.xml'  #should be .xml\n",
    "language = 'eng'\n",
    "#language = 'chinese'\n",
    "lucem_illud.make_TalkbankXML(df_transcript, output_fname, language = language )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25e0Y8F_u4Tz"
   },
   "source": [
    "Run Bayesian Echo Chamber to get estimation.\n",
    "\n",
    "- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n",
    "- Larger \"Vocab_size\" (see below) will cost more time\n",
    "- Larger \"sampling_time\" will also consume more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6omtn8zhu4T0"
   },
   "outputs": [],
   "source": [
    "Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n",
    "sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n",
    "lucem_illud.bec_run(output_fname, Vocab_size, language, sampling_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HFEqCj5u4T0"
   },
   "source": [
    "## <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that perform a similar social similarity or influence analysis on a dataset relevant to your final project (__or one from ConvoKit__). Create relationships between actors in a network based on your dataset (e.g., person to person or document to document), and perform analyses that interrogate the structure of their interactions, similarity, and/or influence on one another. (For example, if relevant to your final project, you could explore different soap operas, counting how many times a character may have used the word love in conversation with another character, and identify if characters in love speak like each other. Or do opposites attract?) What does that analysis and its output reveal about the relative influence of each actor on others? What does it reveal about the social game being played?\n",
    "\n",
    "<font color=\"red\">Stretch 1:\n",
    "Render the social network with weights (e.g., based on the number of scenes in which actors appear together), then calculate the most central actors in the show.Realtime output can be viewed in shell.\n",
    "\n",
    "<font color=\"red\">Stretch 2:\n",
    "Implement more complex measures of similarity based on the papers you have read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uKnCDS1u4T0"
   },
   "source": [
    "# Text Generation & Converstaions via LangChain\n",
    "\n",
    "We can make use of the LLMs we learned in week 6 to do text generation, where the model takes one or multiple places in a conversation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. We mainly use the package LangChain to interact with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTDgRy0jKDkP"
   },
   "source": [
    "## Overview:\n",
    "- Installation and setup (old packages!)\n",
    "- LLMs (Open AI and Open Source)\n",
    "- Prompt Templates and Chains (Smooth communication between models)\n",
    "- Agents and Tools, Memory, Documents\n",
    "- Simulations and Conversations\n",
    "- Simulacra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WGtOYYTKfz3"
   },
   "source": [
    "## Installation\n",
    "\n",
    "In the rapidly evolving landscape of LLMs, packages and libraries like LangChain are frequently updated to leverage the latest advancements. We've chosen this particular version of LangChain for its ease of use in a Colab notebook, effectively demonstrating the power of LLMs as simulators. However, it's important to note that this code is a year old. If you're planning to use it in a production environment, it will be essential to update and adapt it to align with the latest versions and best practices in the field, ensuring compatibility and optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "id": "bcrn7QRyQXGj",
    "outputId": "a9758421-42ab-4c6f-aecc-c90b3da1953b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langchain==0.0.316\n",
      "  Downloading langchain-0.0.316-py3-none-any.whl (1.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.9.3)\n",
      "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.316)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.316)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.43 (from langchain==0.0.316)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.316)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.316) (3.0.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (23.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain\n",
      "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.316 langsmith-0.0.92 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "var0F2Glwnv0",
    "outputId": "1fe07d28-2378-4fc2-e6fa-4c20adcfa4c4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkGGSdmtta6s"
   },
   "source": [
    "## LLMs\n",
    "\n",
    "A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OFLmmrtejCl"
   },
   "source": [
    "### Open AI via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RlxEmS1CaM5v",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "outputId": "a34bf612-b770-4c3c-9657-996b93f48855"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/226.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m225.3/226.7 kB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m226.7/226.7 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/75.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.6/75.6 kB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.8/77.8 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
      "Installing collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "pY09s9cmZ6nQ",
    "outputId": "8e1b8282-7302-4256-cc38-b743c643d837"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-40-7eb03046ba5e>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mllm_openai\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOpenAI\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemperature\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"gpt-3.5-turbo-instruct\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"What would be a good company name for a company that makes colorful socks?\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mllm_openai\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    864\u001B[0m             )\n\u001B[1;32m    865\u001B[0m         return (\n\u001B[0;32m--> 866\u001B[0;31m             self.generate(\n\u001B[0m\u001B[1;32m    867\u001B[0m                 \u001B[0;34m[\u001B[0m\u001B[0mprompt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    868\u001B[0m                 \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    644\u001B[0m                 )\n\u001B[1;32m    645\u001B[0m             ]\n\u001B[0;32m--> 646\u001B[0;31m             output = self._generate_helper(\n\u001B[0m\u001B[1;32m    647\u001B[0m                 \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_managers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    648\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    532\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mrun_manager\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrun_managers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    533\u001B[0m                 \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_llm_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 534\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    535\u001B[0m         \u001B[0mflattened_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    536\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmanager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflattened_output\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_managers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflattened_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    519\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    520\u001B[0m             output = (\n\u001B[0;32m--> 521\u001B[0;31m                 self._generate(\n\u001B[0m\u001B[1;32m    522\u001B[0m                     \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m                     \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001B[0m in \u001B[0;36m_generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    399\u001B[0m                 )\n\u001B[1;32m    400\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 401\u001B[0;31m                 response = completion_with_retry(\n\u001B[0m\u001B[1;32m    402\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprompt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_prompts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    403\u001B[0m                 )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001B[0m in \u001B[0;36mcompletion_with_retry\u001B[0;34m(llm, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    107\u001B[0m ) -> Any:\n\u001B[1;32m    108\u001B[0m     \u001B[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m     \u001B[0mretry_decorator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_retry_decorator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mllm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mretry_decorator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001B[0m in \u001B[0;36m_create_retry_decorator\u001B[0;34m(llm, run_manager)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m     errors = [\n\u001B[0;32m---> 92\u001B[0;31m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTimeout\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAPIError\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAPIConnectionError\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm_openai(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YNn2fvVY1iX"
   },
   "outputs": [],
   "source": [
    "# if you wish, you can also choose to use GPT-4, which is better. But the way to construct message is a bit different\n",
    "# https://python.langchain.com/docs/integrations/chat/openai\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0.9, model_name=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-z3EauZena2"
   },
   "source": [
    "### Using other open-source LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "idkq_aVyaceF",
    "outputId": "140d07a4-6481-412e-c212-91a22aa3e69a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4DKOWjyaRmO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmtH72oCaU32"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uK5TtJPc49I",
    "outputId": "e0b4e96c-5f09-44dd-9664-7c77fba6275e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Our current thought is ColourSock. Also, if you have any tips for branding, that would be helpful as well!\n",
      "\n",
      "A name that describes exactly what the products are is fine, but you should probably have another name that is your \"brand name.\" The brand name can emphasize the colorful and playful aspects, while not necessarily having to tie it in with the type of product.\n",
      "For example, you could create a name like \"HappySocks\" or\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/google/flan-t5-xl\n",
    "llm_mistral = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "print(llm_mistral(prompt = \"What would be a good company name for a company that makes colorful socks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7kSceuAq1Bn"
   },
   "source": [
    "## Personalities and Options\n",
    "\n",
    "In the `get_meal_options` function, the integration of history and context prompts plays a crucial role in shaping the AI's conversation. The `personality_template`, which serves as a context prompt, establishes the AI's expertise in vegetarian, Italian cuisine, and healthy eating. This template sets the stage for the conversation, informing the AI about its role and area of specialization. It's akin to giving the AI a 'character' or 'personality' to adhere to during the interaction, ensuring that its responses are aligned with this predefined context.\n",
    "\n",
    "The history prompts, represented by `example_human_history` and `example_ai_history`, simulate a prior interaction between the human and the AI, adding depth to the conversation's backstory. These prompts are crucial for creating a more natural and realistic interaction, as they give the impression that the conversation is ongoing rather than starting from scratch. When combined with the context prompt in `ChatPromptTemplate`, they create a comprehensive conversation framework. This framework effectively guides the AI's responses to be consistent with both the established personality (context) and the implied history of the interaction, leading to more coherent and relevant meal suggestions for breakfast, lunch, and dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wyMylKqhu-IL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "aa86df35-8a7d-49d3-d89c-73f49a6daf6c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "hDIMf8akfE9a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "7aefa3b1-a93c-4c0c-d192-4d96a1b34c67"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_meal_options(personality_template, human_history=\"\", ai_history=\"\"):\n",
    "    # Initialize ChatOpenAI with OpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # Replace with your API key\n",
    "\n",
    "\n",
    "    example_human_history = HumanMessagePromptTemplate.from_template(human_history)\n",
    "    example_ai_history = AIMessagePromptTemplate.from_template(ai_history)\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(personality_template)\n",
    "\n",
    "    human_template=\"{input}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    # Create ChatPromptTemplate\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human_history, example_ai_history, human_message_prompt])\n",
    "    # Initialize the conversation chain\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "    # Ask for meal options\n",
    "    meals = ['breakfast', 'lunch', 'dinner']\n",
    "    meal_options = {}\n",
    "    for meal in meals:\n",
    "        response = chain.run(f\"What are some good {meal} options for someone who loves vegetarian, Italian cuisine and prefers healthy options?\")\n",
    "        meal_options[meal] = response\n",
    "\n",
    "    return meal_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vywW6mdswKSA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "f01c4b33-f80d-41a6-c283-ff99f9059d2c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Define the personality based on food preferences\n",
    "personality_template = \"\"\"\n",
    "The following is a conversation with an AI who is an expert in vegetarian, Italian cuisine and healthy eating options.\n",
    "AI: I am here to provide creative and healthy vegetarian meal suggestions. Ask me anything about vegetarian food!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "i2z822LLv6iY",
    "outputId": "c58978b3-0574-4501-fcd2-07d8aac5a1a5"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-45-d8969f380e70>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Call the function to get meal options\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmeal_options\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_meal_options\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpersonality_template\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmeal_options\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-43-e23e61b1878a>\u001B[0m in \u001B[0;36mget_meal_options\u001B[0;34m(personality_template, human_history, ai_history)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mmeal_options\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mmeal\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmeals\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"What are some good {meal} options for someone who loves vegetarian, Italian cuisine and prefers healthy options?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0mmeal_options\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmeal\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    501\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    502\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"`run` supports only one positional argument.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 503\u001B[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[0m\u001B[1;32m    504\u001B[0m                 \u001B[0m_output_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m             ]\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    306\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 308\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    309\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m             outputs = (\n\u001B[0;32m--> 302\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    303\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     91\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCallbackManagerForChainRun\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     ) -> Dict[str, str]:\n\u001B[0;32m---> 93\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_outputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    101\u001B[0m         \u001B[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m         \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprep_prompts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m         return self.llm.generate_prompt(\n\u001B[0m\u001B[1;32m    104\u001B[0m             \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m             \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001B[0m in \u001B[0;36mgenerate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m     ) -> LLMResult:\n\u001B[1;32m    464\u001B[0m         \u001B[0mprompt_messages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_messages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 465\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_messages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    467\u001B[0m     async def agenerate_prompt(\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    353\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mrun_managers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    354\u001B[0m                     \u001B[0mrun_managers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_llm_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 355\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    356\u001B[0m         flattened_outputs = [\n\u001B[1;32m    357\u001B[0m             \u001B[0mLLMResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgenerations\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerations\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mllm_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mllm_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    343\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    344\u001B[0m                 results.append(\n\u001B[0;32m--> 345\u001B[0;31m                     self._generate_with_cache(\n\u001B[0m\u001B[1;32m    346\u001B[0m                         \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    347\u001B[0m                         \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001B[0m in \u001B[0;36m_generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    496\u001B[0m                 )\n\u001B[1;32m    497\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 498\u001B[0;31m                 return self._generate(\n\u001B[0m\u001B[1;32m    499\u001B[0m                     \u001B[0mmessages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    500\u001B[0m                 )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001B[0m in \u001B[0;36m_generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    358\u001B[0m         \u001B[0mmessage_dicts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_message_dicts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m         \u001B[0mparams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m         response = self.completion_with_retry(\n\u001B[0m\u001B[1;32m    361\u001B[0m             \u001B[0mmessages\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmessage_dicts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001B[0m in \u001B[0;36mcompletion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m     ) -> Any:\n\u001B[1;32m    292\u001B[0m         \u001B[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 293\u001B[0;31m         \u001B[0mretry_decorator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_retry_decorator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    294\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0mretry_decorator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001B[0m in \u001B[0;36m_create_retry_decorator\u001B[0;34m(llm, run_manager)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m     errors = [\n\u001B[0;32m---> 73\u001B[0;31m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTimeout\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAPIError\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAPIConnectionError\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "# Call the function to get meal options\n",
    "meal_options = get_meal_options(personality_template)\n",
    "print(meal_options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O-7dO1htdO4"
   },
   "source": [
    "## Prompt Templates and Chaining\n",
    "\n",
    "LangChain faciliates prompt management and optimization.\n",
    "\n",
    "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.\n",
    "\n",
    "Prompt templates in LangChain are pre-defined formats or structures for creating prompts that are sent to a language model. These templates help in standardizing and automating the way you interact with the model, ensuring consistency and potentially improving the quality of responses. They are particularly useful when you need to generate prompts that follow a specific format or include certain fixed elements.\n",
    "\n",
    "For example, you might have a prompt template for a customer service bot where each prompt starts with a greeting, includes the user's question, and ends with a closing statement. By using a template, you can easily insert the user's specific question into the prompt without having to manually format the entire prompt each time.\n",
    "\n",
    "This can also be useful if you are conducting a research experiment to consistently measure responses, or for annotation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FDS9IDRapOt"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtIaMPKVx_Q7"
   },
   "outputs": [],
   "source": [
    "# Define the template for sarcasm annotation\n",
    "sarcasm_template = \"\"\"\n",
    "Analyze the following text to determine if it is sarcastic or not.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Is the text sarcastic? Provide a brief explanation for your decision:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBOCff6ryBGe"
   },
   "outputs": [],
   "source": [
    "# Create a PromptTemplate instance with the defined template\n",
    "sarcasm_prompt = PromptTemplate(template=sarcasm_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DO77wA568zLv",
    "outputId": "29528112-0688-4452-aa54-eaf642075bfe"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], template='\\nAnalyze the following text to determine if it is sarcastic or not.\\n\\nText: \"{text}\"\\n\\nIs the text sarcastic? Provide a brief explanation for your decision:\\n')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "sarcasm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yzpc_0aHHeE"
   },
   "outputs": [],
   "source": [
    "# Example text to be analyzed\n",
    "example_text = \"Oh great, another Monday morning!\"\n",
    "\n",
    "# Format the prompt with the specific text\n",
    "prompt_text = sarcasm_prompt.format(text=example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "on8ubh3kt7oD",
    "outputId": "e150daed-fb78-4338-c6b2-49028a6a923b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nAnalyze the following text to determine if it is sarcastic or not.\\n\\nText: \"Oh great, another Monday morning!\"\\n\\nIs the text sarcastic? Provide a brief explanation for your decision:\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwtCx_aAghQi",
    "outputId": "fbb52169-d19a-4e60-9b4d-a9565317731a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Yes, the text is likely to be sarcastic. The use of \"Oh great\" before mentioning Monday morning suggests a negative connotation towards the start of the work week. This tone is often associated with sarcasm, as the speaker is expressing the opposite of what they actually mean.\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(prompt_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zw1KlSeuUOY"
   },
   "source": [
    "## Chains\n",
    "\n",
    "Chaining refers to the process of using the output from one model as the input to another model, or even as input back into the same model, in a sequential manner. This allows for more complex interactions and workflows that go beyond single-turn question-answer formats.\n",
    "\n",
    "In the context of LangChain, chaining can be used to create multi-step processes or workflows. For instance, you might first use a language model to generate a draft response to a user's query, then pass this draft through another model (or the same model) for further refinement, summarization, or translation.\n",
    "\n",
    "Chaining can be powerful in scenarios where a single interaction with a model is insufficient to achieve the desired outcome. It enables more sophisticated applications, such as:\n",
    "\n",
    "Iterative Refinement: Where the output of a model is refined over several iterations to improve quality or detail.\n",
    "Multi-Model Workflows: Where different models are used for different tasks in a sequence, like one model generating content and another summarizing it.\n",
    "Feedback Loops: Where the output of a model is fed back as input to the same model, possibly with modifications, to simulate a conversation or to gradually steer the model towards a specific goal.\n",
    "In LangChain, chaining is facilitated by its architecture, which allows for easy integration and sequencing of different models and processes.\n",
    "\n",
    "Below is the simplest example of such a chain -> simply passing a prompt and model and then \"running\" will pass the message to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eE6n-jbAuOxt",
    "outputId": "d1444b66-aeda-4c7e-a51f-1dc7e32eec74"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Yes, the text is sarcastic. The speaker is using an exaggerated and sarcastic tone to express their dissatisfaction with the idea of working hard for very little pay and the idea that class distinctions are a positive aspect of society.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=sarcasm_prompt, llm=llm_openai)\n",
    "\n",
    "question = \"Oh, awesome, I love working hard and making minimum wage. Class distinctions are a good thing.\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgRYGSrTEGDZ"
   },
   "outputs": [],
   "source": [
    "text_gen_template = \"Write a creative, humorous and sarcastic statement about {text}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y2p6rqsHJ7l"
   },
   "outputs": [],
   "source": [
    "prompt_gen = PromptTemplate(template=text_gen_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GvOyi8pEGGI"
   },
   "outputs": [],
   "source": [
    "llm_chain_gen = LLMChain(prompt=prompt_gen, llm=llm_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kgZmY4vEGI9"
   },
   "outputs": [],
   "source": [
    "sarcastic_subject = \"Mondays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "BNa_umQ5HmD3",
    "outputId": "0302b03b-0f55-4da1-a301-712d5655c49e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n\\nUgh... It's Monday again! That delightful day where dreams of weekend freedom shatter as reality comes crashing down. The perfect time to relish in the joy of alarms, traffic jams, and the ever-growing mountain of emails from people who apparently have nothing better to do on the weekends than to think up new ways to bother you. Mondays: When the sweet taste of freedom morphs into the bitter tang of responsibility, and Saturday's\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "llm_chain_gen.run(sarcastic_subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp-UlOK0bMVQ"
   },
   "source": [
    "## Agents and Tools\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "\n",
    "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "- LLM: The language model powering the agent.\n",
    "- Agent: The agent to use.\n",
    "\n",
    "Tools: https://python.langchain.com/docs/modules/agents/tools/\n",
    "\n",
    "Agent Types: https://python.langchain.com/docs/modules/agents/agent_types/\n",
    "\n",
    "Consider this 'random' request, and use it to imagine related tasks for your own research project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79JcjhFXwv0J"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOSpaurEb1MR",
    "outputId": "c849cdc1-d73f-4b2d-d970-a5a80222cb9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=5441d70d5409dca9000f9daea485ebc09c488d801664f104a6b70f2cbb6ca4bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgV4kny1bgy1"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQUOsWLrbjKv"
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "M8Rob2Wsb_l9",
    "outputId": "f5010b7e-dbaf-4b0e-fb08-5b12afbff2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m We can use Wikipedia to find the year the film was released and a Calculator to raise it to a power.\n",
      "Action: Wikipedia\n",
      "Action Input: The Departed\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001B[36;1m\u001B[1;3mPage: The Departed\n",
      "Summary: The Departed is a 2006 American crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
      "The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
      "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n",
      "It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It became Scorsese's first and, to date, only personal Oscar win; Wahlberg was also nominated for Best Supporting Actor. The film also received six nominations at the 64th Golden Globe Awards, six nominations at the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards. DiCaprio was nominated for Golden Globe Award for Best Actor – Motion Picture Drama (also nominated that year in the same category for Blood Diamond), BAFTA Award for Best Actor in a Leading Role and Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Supporting Role for his performance.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m The film was released in 2006 and we can use a calculator to raise it to the power of 0.43.\n",
      "Action: Calculator\n",
      "Action Input: 2006 ^ 0.43\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 26.30281917656938\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer.\n",
      "Final Answer: 26.30281917656938\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'26.30281917656938'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prqby8Wb9wlx"
   },
   "source": [
    "## <font color=\"red\">*Exercise 3*</font>\n",
    "\n",
    "<font color=\"red\">Review the documentation for tools and agents from LangChain . Utilize at least two tools with appropriate agents discovered during your review to construct a chain addressing questions pertinent to your final project. If your project dataset is unsuitable for this task, select an alternative small-sized dataset for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wMttXM-CuPK"
   },
   "source": [
    "## Memory\n",
    "\n",
    "We can use the history and system prompts to create a memory (of sorts) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5Rk-GIXUwdB"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pavb7kYhPQnq"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "def create_system_message_prompt():\n",
    "    \"\"\" Creates a system message prompt\"\"\"\n",
    "    personality_template = f\"\"\"\n",
    "    The following is a conversation with an AI assistant.\n",
    "    \"\"\"\n",
    "    return SystemMessagePromptTemplate.from_template(personality_template)\n",
    "\n",
    "def create_chat_prompt(human_history, ai_history):\n",
    "    \"\"\" Creates a chat prompt template with human history, and AI history. \"\"\"\n",
    "    messages = []\n",
    "    system_message_prompt = create_system_message_prompt()\n",
    "\n",
    "    for h, a in zip(human_history, ai_history):\n",
    "      messages.append(HumanMessagePromptTemplate.from_template(h))\n",
    "      messages.append(AIMessagePromptTemplate.from_template(a))\n",
    "\n",
    "    messages.append(HumanMessagePromptTemplate.from_template(\"{input}\"))\n",
    "    return ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "def query_chain(chain, input_text):\n",
    "    \"\"\" Queries the conversation chain with the given input. \"\"\"\n",
    "    return chain.run(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRIUPj5aUx35",
    "outputId": "93d79e89-583f-4ca6-b176-f1e8d15bfcd0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "AI: Noted. Your favorite color is blue.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "human_history = []\n",
    "ai_history = []\n",
    "\n",
    "chat_prompt = create_chat_prompt(human_history, ai_history)\n",
    "\n",
    "# Initialize the conversation chain\n",
    "chain = LLMChain(llm=llm_openai, prompt=chat_prompt)\n",
    "\n",
    "# Query the chain\n",
    "human_input =  \"Please take a note that my favorite color is blue.\"\n",
    "ai_response = query_chain(chain, human_input)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKidm0CyOdJS",
    "outputId": "42077cf9-f483-4ec4-8299-5c42dee42942"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "AI: Your favorite color is blue. \n"
     ]
    }
   ],
   "source": [
    "human_history.append(human_input)\n",
    "ai_history.append(ai_response)\n",
    "\n",
    "chat_prompt = create_chat_prompt(human_history, ai_history)\n",
    "\n",
    "# Initialize the conversation chain\n",
    "chain = LLMChain(llm=llm_openai, prompt=chat_prompt)\n",
    "\n",
    "# Query the chain\n",
    "human_input =  \"Could you please remind me what is my favorite color?\"\n",
    "ai_response = query_chain(chain, human_input)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpd1gMPnnZnT"
   },
   "source": [
    "## Simulation and Conversations\n",
    "\n",
    "Above we showcased a toy LLM conversation between Human and AI with memories. We can go beyond the notion of conversation between AI and Human. We can make both participants AIs who think they are conversing with humans! In other words, for example, we can make AI no.1 play the role of Trump and AI no.2 play the role of Biden, and both of them 'think' they're talking to the real Trump/Biden!\n",
    "\n",
    "Let's implement this idea using questions from the 2020 predidential election debate as templates. Different from the hard coding way of implementing memory, below we use the ConversationBufferMemory class from langchain to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juu-QlSfsLxG"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7A6h7mkbyBv"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\", max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4t1qLnw_koX"
   },
   "outputs": [],
   "source": [
    "Trump_memory = ConversationBufferMemory(human_prefix='Trump', ai_prefix='Biden')\n",
    "Biden_memory = ConversationBufferMemory(human_prefix='Biden', ai_prefix='Trump')\n",
    "\n",
    "Trump_template = \"\"\"Imagine you're the Democrats presidential candidate Joe Biden. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Donald Trump. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Trump: {input}\n",
    "Biden:\"\"\"\n",
    "Biden_template = \"\"\"\"Imagine you're the Republican presidential candidate Donald Trump. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Joe Biden. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Biden: {input}\n",
    "Trump:\"\"\"\n",
    "\n",
    "# By default, models recognize themselves as AIs. So we need to let them do role play\n",
    "# and convince them that they're talking to real Trump/Biden (though they're not)\n",
    "\n",
    "TRUMP_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Trump_template)\n",
    "BIDEN_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Biden_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhvM6InzsL3r"
   },
   "outputs": [],
   "source": [
    "Trump_conversation = ConversationChain(\n",
    "    prompt=TRUMP_PROMPT,\n",
    "    llm=llm_openai,\n",
    "    verbose=False,\n",
    "    memory=Trump_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQZ2xLvaJBKE"
   },
   "outputs": [],
   "source": [
    "# suppose Biden starts first\n",
    "biden_response = Trump_conversation.predict(input='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "1pXpwpSNa_Yk",
    "outputId": "599d5d85-e9c1-4562-976c-ac2820249c1d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Well, first of all, thank you for having me here tonight for this important debate. The issue of selecting a Supreme Court nominee in an election year is a critical one, and I believe that my position is the correct one.\\n\\nYou see, the Constitution is clear on this matter. In Article II, Section 2, it states that the President shall nominate Supreme Court Justices with the advice and consent of the Senate. It does not say anything about the timing of when this should occur.\\n\\nIn fact, history has shown us that 29 Supreme Court vacancies have arisen during an election year, and in 17 of those cases, the President has made a nomination. This is not a new or unprecedented situation.\\n\\nFurthermore, just four years ago, when Justice Scalia passed away, President Obama nominated Judge Merrick Garland to fill the vacancy. The Senate refused to even hold a hearing, citing the upcoming election. Now, they want to go against their own precedent and push through a nominee in an election year.\\n\\nBut the American people deserve a say in who sits on the highest court in the land. They are the ones who will be affected by the decisions made by the Supreme Court. And in this election year, they should have the opportunity to vote for a President who will make appointments to the Court.\\n\\nI believe that this is the fair and just approach. And I trust the American people to make their voices heard in November. Thank you.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "biden_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kFjxUnDI1Fp"
   },
   "outputs": [],
   "source": [
    "Biden_conversation = ConversationChain(\n",
    "    prompt=BIDEN_PROMPT,\n",
    "    llm=llm_openai,\n",
    "    verbose=False,\n",
    "    memory=Biden_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emFitHWQXl1T"
   },
   "outputs": [],
   "source": [
    "trump_response = Biden_conversation.predict(input=biden_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "e-9ZcknccNku",
    "outputId": "d2ccc5b5-32a9-497e-cd2f-4b8e5ef83711"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\" Thank you. First, let me say that I am honored to be here tonight and to have this opportunity to speak directly to the American people.\\n\\nLet me be clear, my position on selecting a Supreme Court nominee in an election year is the correct one. And let me tell you why.\\n\\nThe Constitution is clear - the President has the power and responsibility to nominate Supreme Court Justices. And let's not forget that I was elected by the American people to serve as their President for four years, not three years and nine months.\\n\\nThe bottom line is, I have a duty to fill any vacancies on the Supreme Court, and I intend to fulfill that duty. I will nominate a highly qualified, constitutional conservative who will uphold our values and protect our rights.\\n\\nAnd let's not forget that the Senate has a responsibility to provide advice and consent on my nominee. They have the power to reject a nominee if they see fit. But they should not refuse to even consider my nominee based on the timing of an election.\\n\\nIn fact, the Senate has confirmed Supreme Court Justices in presidential election years before. In 1988, President Reagan nominated Justice Kennedy and he was confirmed by a Democratic-controlled Senate. So why should it be any different now?\\n\\nI believe we should move forward with the nomination process and let the Senate do their job. The American people elected us to do a job, and that includes filling vacancies on the Supreme Court. Delaying this process would do a disservice to the American people and undermine the integrity of the Court.\\n\\nI will continue to fulfill my duty as President and nominate a strong and qualified candidate for the Supreme Court. Thank you.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "trump_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot9bFycQcfC9"
   },
   "source": [
    "Well, that seems too nice to be Trump. Anyway, now we can make the debate run by feeding Trump agent's and Biden agent's response to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlITuIPVncVi"
   },
   "source": [
    "## Simulacra\n",
    "\n",
    "One of the most influential generative LLM works in 2023 is the [Generative Agents: Interactive Simulacra of Human Behavior](https://dl.acm.org/doi/abs/10.1145/3586183.3606763) from Stanford. In this paper, the authors constructed a small town with 25 agents driven by ChatGPT:\n",
    "> Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents—computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architec- ture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behav- iors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.\n",
    "\n",
    "<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/1.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/1.png?raw=true\" style=\"width:500px\">\n",
    "\n",
    "### Agent System\n",
    "\n",
    "In the paper, the agents have memory and can reflect. Agents can to remember, retrieve, reflect, interact with other agents, and plan through dynamically evolving circumstances.\n",
    "\n",
    "<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/2.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/2.png?raw=true\" style=\"width:500px\">\n",
    "\n",
    "They open-sourced the code on [GitHub](https://github.com/joonspk-research/generative_agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoBKxJNs_8Gh"
   },
   "source": [
    "### Memory and Information Retrieval\n",
    "\n",
    "Memory and Information Retrieval is the key to set up the whole town, along with ChatGPT, making agents differnent from those simple rule-based agents.\n",
    "\n",
    ">Approach: The memory stream maintains a comprehensive record of the agent’s experience. It is a list of memory objects, where each object contains a natural language description, a creation times- tamp and a most recent access timestamp. The most basic element of the memory stream is an observation, which is an event directly perceived by an agent. Common observations include behaviors performed by the agent themselves, or behaviors that agents per- ceive being performed by other agents or non-agent objects. For instance, Isabella Rodriguez, who works at a coffee shop, might ac- crue the following observations over time: (1) Isabella Rodriguez is setting out the pastries, (2) Maria Lopez is studying for a Chemistry test while drinking coffee, (3) Isabella Rodriguez and Maria Lopez are conversing about planning a Valentine’s day party at Hobbs Cafe, (4) The refrigerator is empty.\n",
    "\n",
    ">Our architecture implements a retrieval function that takes the agent’s current situation as input and returns a subset of the mem- ory stream to pass on to the language model. There are many pos- sible implementations of a retrieval function, depending on what it is important that the agent consider when deciding how to act. In our context, we focus on three main components that together produce effective results.\n",
    "\n",
    "> *Recency* assigns a higher score to memory objects that were re- cently accessed, so that events from a moment ago or this morning are likely to remain in the agent’s attentional sphere. In our im- plementation, we treat recency as an exponential decay function over the number of sandbox game hours since the memory was last retrieved. Our decay factor is 0.99.\n",
    "\n",
    "> *Importance* distinguishes mundane from core memories, by as- signing a higher score to those memory objects that the agent believes to be important. For instance, a mundane event such as eating breakfast in one’s room would yield a low importance score, whereas a breakup with one’s significant other would yield a high score. There are again many possible implementations of an im- portance score; we find that directly asking the language model to output an integer score is effective.\n",
    "\n",
    "> *Relevance* assigns a higher score to memory objects that are related to the current situation. What is relevant depends on the answer to, “Relevant to what?”, so we condition relevance on a query memory. If the query, for example, is that a student is dis- cussing what to study for a chemistry test with a classmate, memory objects about their breakfast should have low relevance, whereas memory objects about the teacher and schoolwork should have high relevance. In our implementation, we use the language model to generate an embedding vector of the text description of each memory. Then, we calculate relevance as the cosine similarity be- tween the memory’s embedding vector and the query memory’s embedding vector.\n",
    "\n",
    "> To calculate the final retrieval score, we normalize the recency, relevance, and importance scores to the range of [0, 1] by min-max scaling. The retrieval function scores all memories as a weighted combination of the three elements:$ score = \\alpha_{recency} * recency + \\alpha_{importance} * importance + \\alpha_{relevance} * relevance$ In our implementation, all $\\alpha$'s are set to 1. The top-ranked memories that fit in the language model’s context window are then included in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGIKO6j6ABH1"
   },
   "source": [
    "<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/3.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/3.png?raw=true\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nfNkMjnKeUk"
   },
   "source": [
    "To give you a sense of how the memory retrieval algorithm works, I will use Trump's tweets data to select several tweets most pertinent to the presidential debate question to prompt the Trump agent to respond to the Biden agent in a different way. In this case, we assume the Trump agent learns how Trump behaves on Twitter and this may help the Trump agent prepare for the presidential debate (Ideally, we should provide Trump's previous television debate and also provide similar materials for the Biden agent to learn. I don't have Biden's tweets at hand, you're welcome to create a tweet-based version Biden agent for the debate!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPI1xqTgu4T0"
   },
   "source": [
    "### Memory Retrieval Exmaple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtjSPKGAhdpH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AroBz4dxu4T0"
   },
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueMbqHgXu4T0"
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjuFjnACu4T0"
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "N6tcRsdNu4T0",
    "outputId": "ad89071b-cb10-43a4-ca14-4a2be909e49a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    source              id_str  \\\n",
       "0       Twitter for iPhone  947824196909961216   \n",
       "1       Twitter for iPhone  947810806430826496   \n",
       "2       Twitter for iPhone  947802588174577664   \n",
       "3       Twitter for iPhone  947614110082043904   \n",
       "4       Twitter for iPhone  947592785519173632   \n",
       "...                    ...                 ...   \n",
       "11894  Twitter for Android  418623389600083968   \n",
       "11895  Twitter for Android  418622703730704384   \n",
       "11896  Twitter for Android  418619537740017664   \n",
       "11897  Twitter for Android  418542137899491328   \n",
       "11898  Twitter for Android  418365112844824576   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Will be leaving Florida for Washington (D.C.) ...   \n",
       "1      Iran is failing at every level despite the ter...   \n",
       "2      The United States has foolishly given Pakistan...   \n",
       "3      HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...   \n",
       "4      As our Country rapidly grows stronger and smar...   \n",
       "...                                                  ...   \n",
       "11894  \"@noahshappy: @realDonaldTrump very true comme...   \n",
       "11895  \"@MeleMallory: @realDonaldTrump You know it's ...   \n",
       "11896  \"@ScreenPlayWritr: @realDonaldTrump Is Al Gore...   \n",
       "11897  This very expensive GLOBAL WARMING bullshit ha...   \n",
       "11898  Today is the first day of the rest of your lif...   \n",
       "\n",
       "                     created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "0     2018-01-01 13:37:52+00:00           8237                      NaN   \n",
       "1     2018-01-01 12:44:40+00:00          14595               25073877.0   \n",
       "2     2018-01-01 12:12:00+00:00          49566                      NaN   \n",
       "3     2017-12-31 23:43:04+00:00          35164                      NaN   \n",
       "4     2017-12-31 22:18:20+00:00          39428                      NaN   \n",
       "...                         ...            ...                      ...   \n",
       "11894 2014-01-02 06:02:48+00:00             24                      NaN   \n",
       "11895 2014-01-02 06:00:04+00:00             56                      NaN   \n",
       "11896 2014-01-02 05:47:30+00:00             20                      NaN   \n",
       "11897 2014-01-02 00:39:56+00:00           6764                      NaN   \n",
       "11898 2014-01-01 12:56:30+00:00            872                      NaN   \n",
       "\n",
       "       favorite_count  is_retweet  \n",
       "0               51473       False  \n",
       "1               53557       False  \n",
       "2              138808       False  \n",
       "3              154769       False  \n",
       "4              157655       False  \n",
       "...               ...         ...  \n",
       "11894              60       False  \n",
       "11895              70       False  \n",
       "11896              38       False  \n",
       "11897            4376       False  \n",
       "11898             644       False  \n",
       "\n",
       "[11899 rows x 8 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-7c6eb303-143e-4f45-a9e0-e7ebcffdb457\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "      <td>2018-01-01 13:37:52+00:00</td>\n",
       "      <td>8237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51473</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "      <td>2018-01-01 12:44:40+00:00</td>\n",
       "      <td>14595</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>53557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "      <td>2018-01-01 12:12:00+00:00</td>\n",
       "      <td>49566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138808</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "      <td>2017-12-31 23:43:04+00:00</td>\n",
       "      <td>35164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154769</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "      <td>2017-12-31 22:18:20+00:00</td>\n",
       "      <td>39428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157655</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418623389600083968</td>\n",
       "      <td>\"@noahshappy: @realDonaldTrump very true comme...</td>\n",
       "      <td>2014-01-02 06:02:48+00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11895</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418622703730704384</td>\n",
       "      <td>\"@MeleMallory: @realDonaldTrump You know it's ...</td>\n",
       "      <td>2014-01-02 06:00:04+00:00</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11896</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418619537740017664</td>\n",
       "      <td>\"@ScreenPlayWritr: @realDonaldTrump Is Al Gore...</td>\n",
       "      <td>2014-01-02 05:47:30+00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11897</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418542137899491328</td>\n",
       "      <td>This very expensive GLOBAL WARMING bullshit ha...</td>\n",
       "      <td>2014-01-02 00:39:56+00:00</td>\n",
       "      <td>6764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4376</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11898</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418365112844824576</td>\n",
       "      <td>Today is the first day of the rest of your lif...</td>\n",
       "      <td>2014-01-01 12:56:30+00:00</td>\n",
       "      <td>872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11899 rows × 8 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c6eb303-143e-4f45-a9e0-e7ebcffdb457')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7c6eb303-143e-4f45-a9e0-e7ebcffdb457 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7c6eb303-143e-4f45-a9e0-e7ebcffdb457');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-bf1de807-4fff-46ab-bb1d-6b66176f13fe\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf1de807-4fff-46ab-bb1d-6b66176f13fe')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-bf1de807-4fff-46ab-bb1d-6b66176f13fe button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11899,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Twitter for Android\",\n          \"Twitter for iPhone\",\n          \"Twitter for Websites\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244532854021539392,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"num_unique_values\": 11899,\n        \"samples\": [\n          479090300728537088,\n          1037107565413511168,\n          1050005011562926080\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11855,\n        \"samples\": [\n          \"\\\"@A_Frog_4_Life: If you're interested in 'balancing' work and pleasure, stop! Instead make your work more pleasurable - Donald Trump\",\n          \"Deals are my art form. Other people paint beautifully or write poetry. I like making deals, preferably big deals. That's how I get my kicks.\",\n          \"I agree getting Tax Cuts approved  is important (we will also get HealthCare), but perhaps no Administration has done more in its first.....\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"num_unique_values\": 11872,\n        \"samples\": [\n          \"2017-07-11 12:57:47+00:00\",\n          \"2014-09-12 11:13:54+00:00\",\n          \"2018-09-20 11:13:44+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12933,\n        \"min\": 0,\n        \"max\": 369530,\n        \"num_unique_values\": 6253,\n        \"samples\": [\n          12932,\n          24533,\n          20487\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 344262537.4058477,\n        \"min\": 759251.0,\n        \"max\": 2231790690.0,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          25073877.0,\n          432895323.0,\n          566952520.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50104,\n        \"min\": 0,\n        \"max\": 616217,\n        \"num_unique_values\": 5940,\n        \"samples\": [\n          154,\n          91711,\n          56790\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OwrXW9diHDT"
   },
   "source": [
    "Trump used to live on Twitter lol.\n",
    "\n",
    "Here, we borrow the idea of the information retrieval algorithm. We define the cosine similarity of the tweet's vector representation to the target post's vector representation as relevance, the normalized score (popularity) as importance, and normalized days apart from the debate day as recency. Then, we pick top 5 tweets for the Trump agent to learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3Nha6jPiDlM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "de918417-7af8-4e34-c458-2e368d391aa9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m149.5/149.5 kB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the most relevant (via transformer embedding distance) piece of prior history (in your memory) that addresses the question at hand:"
   ],
   "metadata": {
    "id": "tad-Zc_iqyq3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSDvxjQil5_a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "6789b9495b854907a57cb7eb87927973",
      "1d38da9fbd2e4b1db9cc5603fe3a73a3",
      "857f7b5e7cce4baf915806ce2b384d83",
      "b20c503e0991429285d8c17d7b493c4b",
      "b288cf876256492ca6c35824e1b2d3d5",
      "fd91e055c24f4847a381f5aade578702",
      "32c5268660084042ae83ed310cf0ad2e",
      "0fd7ed2927fc4890bb75cec17b711f86",
      "0a5c780f67d6434887cd34bb925ade86",
      "03494fff43374996954b227fdb687b3f",
      "2acd47dcb9004f0e922c514ed8183807",
      "409ac364e0324a769073e54e5d1da8bf",
      "f92eb7ad55034adea7a85102dac93f49",
      "5591ca43cb264d7dbe22c092817d0982",
      "88ecefab67254490bb84fd3c0f135dba",
      "9528395df8fe4da084a3e411f6e12b9c",
      "ca1c75ad3fa240cfad20a56fba2fc99c",
      "6440fe73609d4ec198d2b14313a54983",
      "d1c75d4e65994b7dabc6b0fb3a3b4ac4",
      "df6158dfaf244706916011b043d5395d",
      "236d0080d81c47ed8c7fa138fd1792ab",
      "d3d29fe74e414faaae96d2ae7bb596a8",
      "e5dfc8221f1c4f0986e88a49bcc9e80a",
      "2f9a7159bf5745a8b66d2ab2d573ce41",
      "52b5c4c9f2b34c16987f0490552aeebd",
      "6d60083f57184a86a6f3ede105a96e9f",
      "effac782b9ce4d5db478af8822ab41b7",
      "70daf950e25f4bb9a189c6b216317de9",
      "2f6f06c188184982be1dd28059018f8b",
      "04498d27b18c4d6d99d64b5bc30640b1",
      "9faec3103be049c0803478c04983c3c1",
      "cbe8a03f63a1401bb118ed733400512d",
      "575e170aba434822aeddc35834a9c137",
      "ecc7567a7a12499ba8dc378f0cf33336",
      "2cabda67ac2c4eddacf02ad3afe16f03",
      "7af29cdb84a54c07a1249577dc39d6e2",
      "136da6f3dd274cf68f3aa72508ae2254",
      "49ba8eebea0348fc95bc726bbf72ac90",
      "2f1fdfdf467c483192af22640593e380",
      "98ac27a5dc5844d38e38a91979ebdf79",
      "9123a0b1b5794acf8666fe2df220e390",
      "adae7841fe404cdca2a3b98b5e35dc6f",
      "4cb4313b05574e898fecec80eeb6c74e",
      "71fd6ed0bfaa45818a9941061d43bdfd",
      "fe243c9237d44d4689ae429de0aa124b",
      "815f80a7001d4aa996adbf16adbaba43",
      "af761c5c168d4ce7a3435c7867d9c51d",
      "4d6a73353aee4aa3b8ef1bf03ffa9b12",
      "fb600237cccd4f7194ac65bfcf037fae",
      "7a9c3403f73b4c7182657bf1b65b994f",
      "d7b6810df0e44afa8f6496129ecc30ab",
      "dfbfd0796c29475c92bccbf2f8082f5c",
      "0a63332cfb6242f5b66667046cfe7771",
      "c0fc505d1b674c818ac81c750eef31e9",
      "a0bf1d94a4194733b4655f8cef1593e3",
      "bd9b058cf6d4454e9d1b151258a10092",
      "8bf29bafc94e48569024a33d734ae911",
      "5bf92412209345989e3d74b061655db9",
      "ffeca8e61aec4ac0ab3599717dd326ab",
      "a4dca51f04234f53979d6e3c3fc906d6",
      "d8851f8f6f88467aabd6f25dc4ea6385",
      "6e9153b487ac4514aea5369a00aad7bf",
      "4765b96088a04f549430653f1996c5ff",
      "2a96bfcd0ff94f3e9bec82f434040277",
      "01250ab780c3468cae030571b89933c0",
      "36e0b821fcd946d9b8eb589cb10bbaa9",
      "4a1ba1dcede8426597fef81bd428d7ef",
      "0279241d8728444eab9cabcd8fc39f12",
      "e328628498bf435aa173df3486f5287e",
      "0f527cf382624bf6b7c5fbd2ba773f1d",
      "d83209d07507487d941f909b3c063786",
      "f7cdfb5d965d4d1fa845b0d6d7f34267",
      "8ce7bb8afff74c7291f908fbabc4964d",
      "8dad9721c3fb410a803bb8edfe868f0b",
      "194ceaf6c8dd4a96bcd1324edb59e4ce",
      "2e07c4f725834afa82a743a71566ec40",
      "01e781df3070433baad362d7bb08b2e1",
      "08a1b4effd0b4aa7a9ec8175a97dc13e",
      "6775e9ef4fa3479f8ec5bd038b88f75c",
      "35e38d0fb8944aa298b006f32bda4b9d",
      "64c44113eba8423a95c3a1bf8cf64167",
      "13575f00bd9d48b9a7322b61f56c702d",
      "ca4fbfb851884131b1847c65c8778c89",
      "f29b36b5a1ab4fc387d4f4478ecd393b",
      "e18dc7cc356f49af96e57f93550acee9",
      "6050b32c7d9b4431ab8caccde40871bc",
      "c77372d15cd54e6695eac372324056e2",
      "01a7be094ebb499b8d43e1bdd47f1acb",
      "98cfb6bc08fa4804b4e4e463fb77b38a",
      "d4ec3a1b57da48d190ba613594947b45",
      "9522edae5f79451dbcc421a07d5b81b3",
      "8744fb699c79462788e35fdb261a1b46",
      "5d8d93dd494447a496c41f49ca3d574a",
      "1c4671f1a1c949c0b1a37c17ec2ed398",
      "a333f6f3447a400db47388e9e10ebd1e",
      "f8a35171bfa34da5b0ea95b47ddce1df",
      "ae76274bfb404d5f8bf9252af89b2323",
      "f68ebe10449e4c3886373f05fe744081",
      "d58fe0dcb67547a2a582ae4fc12c0ab9",
      "bcf7d1545b7440bca0c77b2e3379afb1",
      "be86bde53c9e4f3a9d677f8702196bc2",
      "0ccc737c9ccb469b963b47d423b1c1a3",
      "45861f8609644ef4bfb0913933729b21",
      "6717201a04084dea97539e9961209384",
      "fca86c801b87418da6d750d50c83b1fd",
      "f4228ef3961540e5bdbb5ba0a9fa9497",
      "dcc99fe5dbf44770b5bc1be05e414638",
      "33f5a64520eb429f856b11918d78b901",
      "6c729ea16a654ccfaac820c47107c071",
      "a9f4417f49654c958b39bc7d95052e4c",
      "d2baf3ed32fe4f58ac2e60bdfd59a802",
      "a71eba2843954958b69bec2f4378572c",
      "7af33cf79e364c6cb7d6a240919e7125",
      "4fe5a07bd98b429fad97e446838eff39",
      "46e346b7b5eb48158fd6d104dabc3b3e",
      "f0d072bfe9d548409aa415ee7679fe8a",
      "46c63eb291ab4ba8a2e3b9196b346720",
      "183a97da33464664b1c619f629f5c816",
      "75cc30e52092405fbd78b63ec501a88c",
      "d308ce7c059549268ee5b90ae7d9b02d",
      "e2e702d303944f9dace4b5887ac0d229"
     ]
    },
    "outputId": "116dbcf3-2a7f-454b-a31e-4c6eed3ade92"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6789b9495b854907a57cb7eb87927973"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "409ac364e0324a769073e54e5d1da8bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5dfc8221f1c4f0986e88a49bcc9e80a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecc7567a7a12499ba8dc378f0cf33336"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe243c9237d44d4689ae429de0aa124b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd9b058cf6d4454e9d1b151258a10092"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a1ba1dcede8426597fef81bd428d7ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08a1b4effd0b4aa7a9ec8175a97dc13e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98cfb6bc08fa4804b4e4e463fb77b38a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcf7d1545b7440bca0c77b2e3379afb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2baf3ed32fe4f58ac2e60bdfd59a802"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# relevance\n",
    "embedding_model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(embedding_model_name, device='cuda')\n",
    "embeddings = model.encode(df['text'].astype(str))\n",
    "target_question = 'Why is your position correct on selecting a Supreme Court nominee in an election year?'\n",
    "similarity = cosine_similarity(embeddings, model.encode([target_question]))\n",
    "df['cosine_similarity'] = similarity\n",
    "df['relevance'] = (df['cosine_similarity'] - df['cosine_similarity'].min()) / (df['cosine_similarity'].max() - df['cosine_similarity'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the most recent memories relevant to that question:"
   ],
   "metadata": {
    "id": "KvAOTAO4rG4U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQdddSTwmWFo"
   },
   "outputs": [],
   "source": [
    "# recency\n",
    "dates = df['created_at'].dt.date\n",
    "target_date = '2020-09-29'\n",
    "df['days_diff'] = (pd.to_datetime(target_date) - pd.to_datetime(dates)).dt.days\n",
    "df['days_diff'] = 0.99 ** df['days_diff'] # decay factor\n",
    "df['recency'] = (df['days_diff'] - df['days_diff'].min()) / (df['days_diff'].max() - df['days_diff'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the most important memory, in this case, represented in terms of the number of likes, shares, and retweets:"
   ],
   "metadata": {
    "id": "Ijz_l9-PrPnv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YRjCNsImiNi"
   },
   "outputs": [],
   "source": [
    "# importance\n",
    "scores = df['favorite_count']\n",
    "df['importance'] = (scores - scores.min()) / (scores.max() - scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "id": "LgYcmPkdzIBt",
    "outputId": "5a17db53-84bd-44c9-9753-f562a9ea8cd4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    source              id_str  \\\n",
       "0       Twitter for iPhone  947824196909961216   \n",
       "1       Twitter for iPhone  947810806430826496   \n",
       "2       Twitter for iPhone  947802588174577664   \n",
       "3       Twitter for iPhone  947614110082043904   \n",
       "4       Twitter for iPhone  947592785519173632   \n",
       "...                    ...                 ...   \n",
       "11894  Twitter for Android  418623389600083968   \n",
       "11895  Twitter for Android  418622703730704384   \n",
       "11896  Twitter for Android  418619537740017664   \n",
       "11897  Twitter for Android  418542137899491328   \n",
       "11898  Twitter for Android  418365112844824576   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Will be leaving Florida for Washington (D.C.) ...   \n",
       "1      Iran is failing at every level despite the ter...   \n",
       "2      The United States has foolishly given Pakistan...   \n",
       "3      HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...   \n",
       "4      As our Country rapidly grows stronger and smar...   \n",
       "...                                                  ...   \n",
       "11894  \"@noahshappy: @realDonaldTrump very true comme...   \n",
       "11895  \"@MeleMallory: @realDonaldTrump You know it's ...   \n",
       "11896  \"@ScreenPlayWritr: @realDonaldTrump Is Al Gore...   \n",
       "11897  This very expensive GLOBAL WARMING bullshit ha...   \n",
       "11898  Today is the first day of the rest of your lif...   \n",
       "\n",
       "                     created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "0     2018-01-01 13:37:52+00:00           8237                      NaN   \n",
       "1     2018-01-01 12:44:40+00:00          14595               25073877.0   \n",
       "2     2018-01-01 12:12:00+00:00          49566                      NaN   \n",
       "3     2017-12-31 23:43:04+00:00          35164                      NaN   \n",
       "4     2017-12-31 22:18:20+00:00          39428                      NaN   \n",
       "...                         ...            ...                      ...   \n",
       "11894 2014-01-02 06:02:48+00:00             24                      NaN   \n",
       "11895 2014-01-02 06:00:04+00:00             56                      NaN   \n",
       "11896 2014-01-02 05:47:30+00:00             20                      NaN   \n",
       "11897 2014-01-02 00:39:56+00:00           6764                      NaN   \n",
       "11898 2014-01-01 12:56:30+00:00            872                      NaN   \n",
       "\n",
       "       favorite_count  is_retweet  cosine_similarity  relevance     days_diff  \\\n",
       "0               51473       False           0.118164   0.367391  4.231214e-05   \n",
       "1               53557       False          -0.038524   0.152157  4.231214e-05   \n",
       "2              138808       False           0.003459   0.209827  4.231214e-05   \n",
       "3              154769       False           0.096200   0.337219  4.188902e-05   \n",
       "4              157655       False           0.100477   0.343095  4.188902e-05   \n",
       "...               ...         ...                ...        ...           ...   \n",
       "11894              60       False           0.023121   0.236835  1.794110e-11   \n",
       "11895              70       False           0.048183   0.271262  1.794110e-11   \n",
       "11896              38       False           0.181863   0.454891  1.794110e-11   \n",
       "11897            4376       False          -0.006410   0.196271  1.794110e-11   \n",
       "11898             644       False          -0.010654   0.190441  1.776169e-11   \n",
       "\n",
       "            recency  importance  \n",
       "0      2.577571e-02    0.083531  \n",
       "1      2.577571e-02    0.086913  \n",
       "2      2.577571e-02    0.225258  \n",
       "3      2.551795e-02    0.251160  \n",
       "4      2.551795e-02    0.255843  \n",
       "...             ...         ...  \n",
       "11894  1.092937e-10    0.000097  \n",
       "11895  1.092937e-10    0.000114  \n",
       "11896  1.092937e-10    0.000062  \n",
       "11897  1.092937e-10    0.007101  \n",
       "11898  0.000000e+00    0.001045  \n",
       "\n",
       "[11899 rows x 13 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-f96ec48f-cd85-4c50-aa4b-c07b609d8ab4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>relevance</th>\n",
       "      <th>days_diff</th>\n",
       "      <th>recency</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "      <td>2018-01-01 13:37:52+00:00</td>\n",
       "      <td>8237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51473</td>\n",
       "      <td>False</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>4.231214e-05</td>\n",
       "      <td>2.577571e-02</td>\n",
       "      <td>0.083531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "      <td>2018-01-01 12:44:40+00:00</td>\n",
       "      <td>14595</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>53557</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.038524</td>\n",
       "      <td>0.152157</td>\n",
       "      <td>4.231214e-05</td>\n",
       "      <td>2.577571e-02</td>\n",
       "      <td>0.086913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "      <td>2018-01-01 12:12:00+00:00</td>\n",
       "      <td>49566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138808</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.209827</td>\n",
       "      <td>4.231214e-05</td>\n",
       "      <td>2.577571e-02</td>\n",
       "      <td>0.225258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "      <td>2017-12-31 23:43:04+00:00</td>\n",
       "      <td>35164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154769</td>\n",
       "      <td>False</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.337219</td>\n",
       "      <td>4.188902e-05</td>\n",
       "      <td>2.551795e-02</td>\n",
       "      <td>0.251160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "      <td>2017-12-31 22:18:20+00:00</td>\n",
       "      <td>39428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157655</td>\n",
       "      <td>False</td>\n",
       "      <td>0.100477</td>\n",
       "      <td>0.343095</td>\n",
       "      <td>4.188902e-05</td>\n",
       "      <td>2.551795e-02</td>\n",
       "      <td>0.255843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418623389600083968</td>\n",
       "      <td>\"@noahshappy: @realDonaldTrump very true comme...</td>\n",
       "      <td>2014-01-02 06:02:48+00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.236835</td>\n",
       "      <td>1.794110e-11</td>\n",
       "      <td>1.092937e-10</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11895</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418622703730704384</td>\n",
       "      <td>\"@MeleMallory: @realDonaldTrump You know it's ...</td>\n",
       "      <td>2014-01-02 06:00:04+00:00</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048183</td>\n",
       "      <td>0.271262</td>\n",
       "      <td>1.794110e-11</td>\n",
       "      <td>1.092937e-10</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11896</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418619537740017664</td>\n",
       "      <td>\"@ScreenPlayWritr: @realDonaldTrump Is Al Gore...</td>\n",
       "      <td>2014-01-02 05:47:30+00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>0.181863</td>\n",
       "      <td>0.454891</td>\n",
       "      <td>1.794110e-11</td>\n",
       "      <td>1.092937e-10</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11897</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418542137899491328</td>\n",
       "      <td>This very expensive GLOBAL WARMING bullshit ha...</td>\n",
       "      <td>2014-01-02 00:39:56+00:00</td>\n",
       "      <td>6764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4376</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.006410</td>\n",
       "      <td>0.196271</td>\n",
       "      <td>1.794110e-11</td>\n",
       "      <td>1.092937e-10</td>\n",
       "      <td>0.007101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11898</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>418365112844824576</td>\n",
       "      <td>Today is the first day of the rest of your lif...</td>\n",
       "      <td>2014-01-01 12:56:30+00:00</td>\n",
       "      <td>872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.010654</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>1.776169e-11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11899 rows × 13 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f96ec48f-cd85-4c50-aa4b-c07b609d8ab4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f96ec48f-cd85-4c50-aa4b-c07b609d8ab4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f96ec48f-cd85-4c50-aa4b-c07b609d8ab4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-22a08adf-ab50-4dcb-a861-c5e651e025de\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22a08adf-ab50-4dcb-a861-c5e651e025de')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-22a08adf-ab50-4dcb-a861-c5e651e025de button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11899,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Twitter for Android\",\n          \"Twitter for iPhone\",\n          \"Twitter for Websites\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244532854021539392,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"num_unique_values\": 11899,\n        \"samples\": [\n          479090300728537088,\n          1037107565413511168,\n          1050005011562926080\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11855,\n        \"samples\": [\n          \"\\\"@A_Frog_4_Life: If you're interested in 'balancing' work and pleasure, stop! Instead make your work more pleasurable - Donald Trump\",\n          \"Deals are my art form. Other people paint beautifully or write poetry. I like making deals, preferably big deals. That's how I get my kicks.\",\n          \"I agree getting Tax Cuts approved  is important (we will also get HealthCare), but perhaps no Administration has done more in its first.....\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"num_unique_values\": 11872,\n        \"samples\": [\n          \"2017-07-11 12:57:47+00:00\",\n          \"2014-09-12 11:13:54+00:00\",\n          \"2018-09-20 11:13:44+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12933,\n        \"min\": 0,\n        \"max\": 369530,\n        \"num_unique_values\": 6253,\n        \"samples\": [\n          12932,\n          24533,\n          20487\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 344262537.4058477,\n        \"min\": 759251.0,\n        \"max\": 2231790690.0,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          25073877.0,\n          432895323.0,\n          566952520.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50104,\n        \"min\": 0,\n        \"max\": 616217,\n        \"num_unique_values\": 5940,\n        \"samples\": [\n          154,\n          91711,\n          56790\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_similarity\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 11849,\n        \"samples\": [\n          0.062280893325805664,\n          0.12882499396800995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 11846,\n        \"samples\": [\n          0.41621166467666626,\n          0.3820353150367737\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0003140963946255719,\n        \"min\": 1.776169032077608e-11,\n        \"max\": 0.0016415501428270432,\n        \"num_unique_values\": 1068,\n        \"samples\": [\n          4.969376171501411e-05,\n          4.8268857545700025e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1913413363561269,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1068,\n        \"samples\": [\n          0.030272449920678113,\n          0.0029404328988659524\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08131021823774882,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 5940,\n        \"samples\": [\n          0.00024991196283127533,\n          0.14882906508583826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we create a function that combines relevance, recency, and importance (here with equal and additive contribution):"
   ],
   "metadata": {
    "id": "N0C4VZu2rd0B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuzvOKSvm-sS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "outputId": "3c702503-3632-42d9-c979-4b896de933fe"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  source               id_str  \\\n",
       "2648  Twitter for iPhone  1077549301449060352   \n",
       "3630  Twitter for iPhone  1045444544068812800   \n",
       "2836  Twitter for iPhone  1071076519584268288   \n",
       "2607  Twitter for iPhone  1079830267274108928   \n",
       "2678  Twitter for iPhone  1076655729820471296   \n",
       "\n",
       "                                                   text  \\\n",
       "2648                                   Merry Christmas!   \n",
       "3630  Judge Kavanaugh showed America exactly why I n...   \n",
       "2836  I am pleased to announce that I will be nomina...   \n",
       "2607  Heads of countries are calling wanting to know...   \n",
       "2678  Brett McGurk, who I do not know, was appointed...   \n",
       "\n",
       "                    created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "2648 2018-12-25 12:59:08+00:00          89255                      NaN   \n",
       "3630 2018-09-27 22:46:17+00:00          84180                      NaN   \n",
       "2836 2018-12-07 16:18:36+00:00          13779                      NaN   \n",
       "2607 2018-12-31 20:02:52+00:00          21030                      NaN   \n",
       "2678 2018-12-23 01:48:23+00:00          19476                      NaN   \n",
       "\n",
       "      favorite_count  is_retweet  cosine_similarity  relevance  days_diff  \\\n",
       "2648          508372       False          -0.025833   0.169590   0.001545   \n",
       "3630          320104       False           0.509458   0.904891   0.000632   \n",
       "2836           65313       False           0.464247   0.842787   0.001290   \n",
       "2607           76721       False           0.292551   0.606938   0.001642   \n",
       "2678           86619       False           0.334501   0.664562   0.001515   \n",
       "\n",
       "       recency  importance     score  \n",
       "2648  0.941480    0.824989  1.936058  \n",
       "3630  0.384896    0.519466  1.809253  \n",
       "2836  0.785678    0.105990  1.734456  \n",
       "2607  1.000000    0.124503  1.731441  \n",
       "2678  0.922745    0.140566  1.727872  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-a8a24485-3b98-48a7-a15f-db0351c407ba\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>relevance</th>\n",
       "      <th>days_diff</th>\n",
       "      <th>recency</th>\n",
       "      <th>importance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1077549301449060352</td>\n",
       "      <td>Merry Christmas!</td>\n",
       "      <td>2018-12-25 12:59:08+00:00</td>\n",
       "      <td>89255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>508372</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.025833</td>\n",
       "      <td>0.169590</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.941480</td>\n",
       "      <td>0.824989</td>\n",
       "      <td>1.936058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3630</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1045444544068812800</td>\n",
       "      <td>Judge Kavanaugh showed America exactly why I n...</td>\n",
       "      <td>2018-09-27 22:46:17+00:00</td>\n",
       "      <td>84180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320104</td>\n",
       "      <td>False</td>\n",
       "      <td>0.509458</td>\n",
       "      <td>0.904891</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.384896</td>\n",
       "      <td>0.519466</td>\n",
       "      <td>1.809253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1071076519584268288</td>\n",
       "      <td>I am pleased to announce that I will be nomina...</td>\n",
       "      <td>2018-12-07 16:18:36+00:00</td>\n",
       "      <td>13779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65313</td>\n",
       "      <td>False</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>0.842787</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.785678</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>1.734456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1079830267274108928</td>\n",
       "      <td>Heads of countries are calling wanting to know...</td>\n",
       "      <td>2018-12-31 20:02:52+00:00</td>\n",
       "      <td>21030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76721</td>\n",
       "      <td>False</td>\n",
       "      <td>0.292551</td>\n",
       "      <td>0.606938</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124503</td>\n",
       "      <td>1.731441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1076655729820471296</td>\n",
       "      <td>Brett McGurk, who I do not know, was appointed...</td>\n",
       "      <td>2018-12-23 01:48:23+00:00</td>\n",
       "      <td>19476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86619</td>\n",
       "      <td>False</td>\n",
       "      <td>0.334501</td>\n",
       "      <td>0.664562</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.922745</td>\n",
       "      <td>0.140566</td>\n",
       "      <td>1.727872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8a24485-3b98-48a7-a15f-db0351c407ba')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a8a24485-3b98-48a7-a15f-db0351c407ba button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a8a24485-3b98-48a7-a15f-db0351c407ba');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ebc40001-8442-4cee-888f-64280023d4b0\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebc40001-8442-4cee-888f-64280023d4b0')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ebc40001-8442-4cee-888f-64280023d4b0 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11899,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Twitter for iPad\",\n          \"Twitter for iPhone\",\n          \"Twitlonger\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244532854021539392,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"num_unique_values\": 11899,\n        \"samples\": [\n          923546192511946752,\n          977159683151712256,\n          543700405604339712\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11855,\n        \"samples\": [\n          \"\\\"@jeddprice: @realDonaldTrump Maybe the USA needs to cancel all the rest of Obama's vacations to actually deal with this.\\\"\",\n          \"\\\"@Drake4444444                    \\nthe only way to fix health care crisis is for you to run for Pres &amp; bring common sense back to govt\\\" Thx.\",\n          \"......MAKE FRANCE GREAT AGAIN!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"num_unique_values\": 11872,\n        \"samples\": [\n          \"2018-04-25 11:53:36+00:00\",\n          \"2014-06-01 00:55:12+00:00\",\n          \"2014-10-15 16:18:27+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12933,\n        \"min\": 0,\n        \"max\": 369530,\n        \"num_unique_values\": 6253,\n        \"samples\": [\n          29452,\n          11846,\n          9254\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 344262537.4058477,\n        \"min\": 759251.0,\n        \"max\": 2231790690.0,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          25073877.0,\n          282578001.0,\n          15084853.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50104,\n        \"min\": 0,\n        \"max\": 616217,\n        \"num_unique_values\": 5940,\n        \"samples\": [\n          37547,\n          70931,\n          59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_similarity\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 11849,\n        \"samples\": [\n          0.02068699151277542,\n          0.18732097744941711\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 11846,\n        \"samples\": [\n          0.32346102595329285,\n          0.46238821744918823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00031409639462557184,\n        \"min\": 1.776169032077608e-11,\n        \"max\": 0.0016415501428270432,\n        \"num_unique_values\": 1068,\n        \"samples\": [\n          2.995395511595202e-11,\n          0.00016766535271071751\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19134133635612688,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1068,\n        \"samples\": [\n          7.427287543041394e-09,\n          0.10213841928363422\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08131021823774882,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 5940,\n        \"samples\": [\n          0.06093145758718114,\n          0.1151071781531506\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2606780769976757,\n        \"min\": 0.011552770506056203,\n        \"max\": 1.9360582680637455,\n        \"num_unique_values\": 11896,\n        \"samples\": [\n          0.5951778898072208,\n          0.33441345896834085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# select top 5\n",
    "df['score'] = df['relevance'] + df['recency'] + df['importance']\n",
    "df = df.sort_values(by='score', ascending=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsFEL0eczIBt"
   },
   "source": [
    "Let's put the top 5 tweets into the prompt (Trump) and re-run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ankzgBhzIBt"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt7RzfPEzIBt"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\", max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cciqx5UbzIBt",
    "outputId": "73660ce9-d1a9-47f8-eb89-b8f238b952a5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Merry Christmas!',\n",
       " 'Judge Kavanaugh showed America exactly why I nominated him. His testimony was powerful, honest, and riveting. Democrats’ search and destroy strategy is disgraceful and this process has been a total sham and effort to delay, obstruct, and resist. The Senate must vote!',\n",
       " 'I am pleased to announce that I will be nominating The Honorable William P. Barr for the position of Attorney General of the United States. As the former AG for George H.W. Bush....',\n",
       " 'Heads of countries are calling wanting to know why Senator Schumer is not approving their otherwise approved Ambassadors!? Likewise in Government lawyers and others are being delayed at a record pace! 360 great and hardworking people are waiting for approval from....',\n",
       " 'Brett McGurk, who I do not know, was appointed by President Obama in 2015. Was supposed to leave in February but he just resigned prior to leaving. Grandstander? The Fake News is making such a big deal about this nothing event!']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df.head(5)['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZHDjsdxzIBt"
   },
   "source": [
    "Great, we saw Judge Kavanaugh and topics like border walls appeared.\n",
    "\n",
    "## NOTE!\n",
    "\n",
    "The code below might be a little confusing in terms of the names as the Trump template has a prompt relating to Biden, and vice versa. This is because when we later construct the conversation chain, we get a \"response\" from the other end - so the Biden response comes from a Trump conversation chain.\n",
    "\n",
    "You can also swap these names around if it makes more sense (note it won't effect the actual functioning!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFrIHTXpzIBt"
   },
   "outputs": [],
   "source": [
    "Trump_memory = ConversationBufferMemory(human_prefix='Trump', ai_prefix='Biden')\n",
    "Biden_memory = ConversationBufferMemory(human_prefix='Biden', ai_prefix='Trump')\n",
    "\n",
    "Trump_template = \"\"\"Imagine you're the Democrats presidential candidate Joe Biden. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Donald Trump. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Trump: {input}\n",
    "Biden:\"\"\"\n",
    "\n",
    "tweets = df.head(5)['text'].tolist()\n",
    "Biden_template = \"\"\"\"Imagine you're the Republican presidential candidate Donald Trump. To give you a better sense of how Trump may give public comments, here're some of his tweets:\n",
    "    1. Merry Christmas!\n",
    "    2. Judge Kavanaugh showed America exactly why I nominated him. His testimony was powerful, honest, and riveting. Democrats’ search and destroy strategy is disgraceful and this process has been a total sham and effort to delay, obstruct, and resist. The Senate must vote!\n",
    "    3. Heads of countries are calling wanting to know why Senator Schumer is not approving their otherwise approved Ambassadors!? Likewise in Government lawyers and others are being delayed at a record pace! 360 great and hardworking people are waiting for approval from....\n",
    "    4. President and Mrs. Obama built/has a ten foot Wall around their D.C. mansion/compound. I agree, totally necessary for their safety and security. The U.S. needs the same thing, slightly larger version!\n",
    "    5. Brett McGurk, who I do not know, was appointed by President Obama in 2015. Was supposed to leave in February but he just resigned prior to leaving. Grandstander? The Fake News is making such a big deal about this nothing event!\n",
    "Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Joe Biden. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Biden: {input}\n",
    "Trump:\"\"\"\n",
    "\n",
    "# By default, models recognize themselves as AIs. So we need to let them do role play\n",
    "# and convince them that they're talking to real Trump/Biden (though they're not)\n",
    "\n",
    "TRUMP_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Trump_template)\n",
    "BIDEN_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Biden_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzDEECwUzIBt"
   },
   "outputs": [],
   "source": [
    "Trump_conversation = ConversationChain(\n",
    "    prompt=TRUMP_PROMPT,\n",
    "    llm=llm_openai,\n",
    "    verbose=False,\n",
    "    memory=Trump_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11cJDrtNzIBt"
   },
   "outputs": [],
   "source": [
    "# suppose Biden starts first\n",
    "biden_response = Trump_conversation.predict(input='')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# use the same biden response\n",
    "biden_response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "dhCvJ_DD3G9L",
    "outputId": "aca5f32a-4678-4a4e-8cf3-1bdff84c0f13"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nThank you for the question. I believe my position on selecting a Supreme Court nominee in an election year is the correct one because it follows the precedent set by past administrations.\\n\\nIn 2016, when President Obama nominated Merrick Garland to the Supreme Court, the Republican-controlled Senate refused to even hold a hearing for him, claiming that it was an election year and the next president should make the nomination. Now, just four years later, they are rushing to confirm a nominee in the final months of an election.\\n\\nThis is a clear display of hypocrisy and goes against the principle of fairness and consistency. The American people deserve to have a say in the direction of the Supreme Court through their vote in this election.\\n\\nFurthermore, rushing through a nomination so close to an election sets a dangerous precedent and undermines the integrity of the Court. We should not be making such important decisions in a rushed and partisan manner.\\n\\nI believe the Senate should follow the same standard that they set in 2016 and allow the next president to make the nomination. This will ensure that the American people have a voice in the process and that the Court remains a fair and impartial body. Thank you.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QE9HEK2UzIBt"
   },
   "outputs": [],
   "source": [
    "Biden_conversation = ConversationChain(\n",
    "    prompt=BIDEN_PROMPT,\n",
    "    llm=llm_openai,\n",
    "    verbose=False,\n",
    "    memory=Biden_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ7GEXuDzIBt"
   },
   "outputs": [],
   "source": [
    "trump_response = Biden_conversation.predict(input=biden_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "MZoIG56hzIBt",
    "outputId": "9e06a592-900b-4407-ab4a-be2f5a3c33c7"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n\\nWell, first of all, let me just say that I am honored to be here with all of you watching tonight. And you know what? I have to agree with Joe on one thing, which is that this is an important decision. And as the President, it is my duty to nominate a Supreme Court Justice who will uphold our Constitution and protect the rights of all Americans.\\n\\nBut let's be real, folks. The Democrats are just trying to play games and delay this process. They don't want to see another conservative Justice on the Court, so they are using this election as an excuse to block my nomination.\\n\\nNow, let me remind everyone that in 2016, the American people spoke loud and clear when they elected me as their president. And they didn't elect me for just three years, they elected me for four years. So it is my responsibility and my right to nominate someone for the Supreme Court, and the Senate has a duty to vote on that nominee.\\n\\nAnd let's talk about the precedent. In the past, there have been 29 times when a vacancy occurred in an election year and the president made a nomination. In 19 of those cases, the Senate confirmed the nominee. So this is not unprecedented, and in fact, it is their constitutional duty to vote on my nominee.\\n\\nAnd let's not forget, we are in a different situation now. We are facing a global pandemic, economic crisis, and civil unrest. We need a fully-staffed Supreme Court to make important decisions and uphold our laws. We can't afford to wait until after the election to fill this vacancy.\\n\\nSo my position is clear. I have the right to make a nomination and the Senate has the responsibility to vote on it. Let's stop playing games and do what is best for the American people. Thank you.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "trump_response"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, the agent's response sounds more like Trump by starting with a harsh response to Biden's statement."
   ],
   "metadata": {
    "id": "-cb2qr4I3jG2"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkoPr9CQu4T2"
   },
   "source": [
    "## <font color=\"red\">*Exercise 4*</font>\n",
    "\n",
    "<font color=\"red\">Use LangChain(you're welcome to not use it) to set up conversations with LLM agents for questions related to your final project (if relevant), or think of a scenario that a simulated conversation could be useful to answer a research question and find a dataset to implement it. What does it reveal about the social game involved with your dataset?\n",
    "\n",
    "<font color=\"red\"> Stretch: Use the idea of memory retrieval(or other methods) to design better templates for the LLM conversation."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cRQvTtPZ442g"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "6789b9495b854907a57cb7eb87927973": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d38da9fbd2e4b1db9cc5603fe3a73a3",
       "IPY_MODEL_857f7b5e7cce4baf915806ce2b384d83",
       "IPY_MODEL_b20c503e0991429285d8c17d7b493c4b"
      ],
      "layout": "IPY_MODEL_b288cf876256492ca6c35824e1b2d3d5"
     }
    },
    "1d38da9fbd2e4b1db9cc5603fe3a73a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd91e055c24f4847a381f5aade578702",
      "placeholder": "​",
      "style": "IPY_MODEL_32c5268660084042ae83ed310cf0ad2e",
      "value": "modules.json: 100%"
     }
    },
    "857f7b5e7cce4baf915806ce2b384d83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fd7ed2927fc4890bb75cec17b711f86",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a5c780f67d6434887cd34bb925ade86",
      "value": 349
     }
    },
    "b20c503e0991429285d8c17d7b493c4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03494fff43374996954b227fdb687b3f",
      "placeholder": "​",
      "style": "IPY_MODEL_2acd47dcb9004f0e922c514ed8183807",
      "value": " 349/349 [00:00&lt;00:00, 10.9kB/s]"
     }
    },
    "b288cf876256492ca6c35824e1b2d3d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd91e055c24f4847a381f5aade578702": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32c5268660084042ae83ed310cf0ad2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fd7ed2927fc4890bb75cec17b711f86": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a5c780f67d6434887cd34bb925ade86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "03494fff43374996954b227fdb687b3f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2acd47dcb9004f0e922c514ed8183807": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "409ac364e0324a769073e54e5d1da8bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f92eb7ad55034adea7a85102dac93f49",
       "IPY_MODEL_5591ca43cb264d7dbe22c092817d0982",
       "IPY_MODEL_88ecefab67254490bb84fd3c0f135dba"
      ],
      "layout": "IPY_MODEL_9528395df8fe4da084a3e411f6e12b9c"
     }
    },
    "f92eb7ad55034adea7a85102dac93f49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca1c75ad3fa240cfad20a56fba2fc99c",
      "placeholder": "​",
      "style": "IPY_MODEL_6440fe73609d4ec198d2b14313a54983",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "5591ca43cb264d7dbe22c092817d0982": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1c75d4e65994b7dabc6b0fb3a3b4ac4",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df6158dfaf244706916011b043d5395d",
      "value": 116
     }
    },
    "88ecefab67254490bb84fd3c0f135dba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_236d0080d81c47ed8c7fa138fd1792ab",
      "placeholder": "​",
      "style": "IPY_MODEL_d3d29fe74e414faaae96d2ae7bb596a8",
      "value": " 116/116 [00:00&lt;00:00, 4.49kB/s]"
     }
    },
    "9528395df8fe4da084a3e411f6e12b9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca1c75ad3fa240cfad20a56fba2fc99c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6440fe73609d4ec198d2b14313a54983": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1c75d4e65994b7dabc6b0fb3a3b4ac4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df6158dfaf244706916011b043d5395d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "236d0080d81c47ed8c7fa138fd1792ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3d29fe74e414faaae96d2ae7bb596a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5dfc8221f1c4f0986e88a49bcc9e80a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f9a7159bf5745a8b66d2ab2d573ce41",
       "IPY_MODEL_52b5c4c9f2b34c16987f0490552aeebd",
       "IPY_MODEL_6d60083f57184a86a6f3ede105a96e9f"
      ],
      "layout": "IPY_MODEL_effac782b9ce4d5db478af8822ab41b7"
     }
    },
    "2f9a7159bf5745a8b66d2ab2d573ce41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70daf950e25f4bb9a189c6b216317de9",
      "placeholder": "​",
      "style": "IPY_MODEL_2f6f06c188184982be1dd28059018f8b",
      "value": "README.md: 100%"
     }
    },
    "52b5c4c9f2b34c16987f0490552aeebd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04498d27b18c4d6d99d64b5bc30640b1",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9faec3103be049c0803478c04983c3c1",
      "value": 10659
     }
    },
    "6d60083f57184a86a6f3ede105a96e9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe8a03f63a1401bb118ed733400512d",
      "placeholder": "​",
      "style": "IPY_MODEL_575e170aba434822aeddc35834a9c137",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 526kB/s]"
     }
    },
    "effac782b9ce4d5db478af8822ab41b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70daf950e25f4bb9a189c6b216317de9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f6f06c188184982be1dd28059018f8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04498d27b18c4d6d99d64b5bc30640b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9faec3103be049c0803478c04983c3c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbe8a03f63a1401bb118ed733400512d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "575e170aba434822aeddc35834a9c137": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecc7567a7a12499ba8dc378f0cf33336": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cabda67ac2c4eddacf02ad3afe16f03",
       "IPY_MODEL_7af29cdb84a54c07a1249577dc39d6e2",
       "IPY_MODEL_136da6f3dd274cf68f3aa72508ae2254"
      ],
      "layout": "IPY_MODEL_49ba8eebea0348fc95bc726bbf72ac90"
     }
    },
    "2cabda67ac2c4eddacf02ad3afe16f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f1fdfdf467c483192af22640593e380",
      "placeholder": "​",
      "style": "IPY_MODEL_98ac27a5dc5844d38e38a91979ebdf79",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "7af29cdb84a54c07a1249577dc39d6e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9123a0b1b5794acf8666fe2df220e390",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_adae7841fe404cdca2a3b98b5e35dc6f",
      "value": 53
     }
    },
    "136da6f3dd274cf68f3aa72508ae2254": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cb4313b05574e898fecec80eeb6c74e",
      "placeholder": "​",
      "style": "IPY_MODEL_71fd6ed0bfaa45818a9941061d43bdfd",
      "value": " 53.0/53.0 [00:00&lt;00:00, 1.08kB/s]"
     }
    },
    "49ba8eebea0348fc95bc726bbf72ac90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f1fdfdf467c483192af22640593e380": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98ac27a5dc5844d38e38a91979ebdf79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9123a0b1b5794acf8666fe2df220e390": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adae7841fe404cdca2a3b98b5e35dc6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4cb4313b05574e898fecec80eeb6c74e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71fd6ed0bfaa45818a9941061d43bdfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe243c9237d44d4689ae429de0aa124b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_815f80a7001d4aa996adbf16adbaba43",
       "IPY_MODEL_af761c5c168d4ce7a3435c7867d9c51d",
       "IPY_MODEL_4d6a73353aee4aa3b8ef1bf03ffa9b12"
      ],
      "layout": "IPY_MODEL_fb600237cccd4f7194ac65bfcf037fae"
     }
    },
    "815f80a7001d4aa996adbf16adbaba43": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a9c3403f73b4c7182657bf1b65b994f",
      "placeholder": "​",
      "style": "IPY_MODEL_d7b6810df0e44afa8f6496129ecc30ab",
      "value": "config.json: 100%"
     }
    },
    "af761c5c168d4ce7a3435c7867d9c51d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfbfd0796c29475c92bccbf2f8082f5c",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a63332cfb6242f5b66667046cfe7771",
      "value": 612
     }
    },
    "4d6a73353aee4aa3b8ef1bf03ffa9b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0fc505d1b674c818ac81c750eef31e9",
      "placeholder": "​",
      "style": "IPY_MODEL_a0bf1d94a4194733b4655f8cef1593e3",
      "value": " 612/612 [00:00&lt;00:00, 17.7kB/s]"
     }
    },
    "fb600237cccd4f7194ac65bfcf037fae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a9c3403f73b4c7182657bf1b65b994f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7b6810df0e44afa8f6496129ecc30ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dfbfd0796c29475c92bccbf2f8082f5c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a63332cfb6242f5b66667046cfe7771": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0fc505d1b674c818ac81c750eef31e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0bf1d94a4194733b4655f8cef1593e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9b058cf6d4454e9d1b151258a10092": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bf29bafc94e48569024a33d734ae911",
       "IPY_MODEL_5bf92412209345989e3d74b061655db9",
       "IPY_MODEL_ffeca8e61aec4ac0ab3599717dd326ab"
      ],
      "layout": "IPY_MODEL_a4dca51f04234f53979d6e3c3fc906d6"
     }
    },
    "8bf29bafc94e48569024a33d734ae911": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8851f8f6f88467aabd6f25dc4ea6385",
      "placeholder": "​",
      "style": "IPY_MODEL_6e9153b487ac4514aea5369a00aad7bf",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "5bf92412209345989e3d74b061655db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4765b96088a04f549430653f1996c5ff",
      "max": 90888945,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a96bfcd0ff94f3e9bec82f434040277",
      "value": 90888945
     }
    },
    "ffeca8e61aec4ac0ab3599717dd326ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01250ab780c3468cae030571b89933c0",
      "placeholder": "​",
      "style": "IPY_MODEL_36e0b821fcd946d9b8eb589cb10bbaa9",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 98.2MB/s]"
     }
    },
    "a4dca51f04234f53979d6e3c3fc906d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8851f8f6f88467aabd6f25dc4ea6385": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e9153b487ac4514aea5369a00aad7bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4765b96088a04f549430653f1996c5ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a96bfcd0ff94f3e9bec82f434040277": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "01250ab780c3468cae030571b89933c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36e0b821fcd946d9b8eb589cb10bbaa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a1ba1dcede8426597fef81bd428d7ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0279241d8728444eab9cabcd8fc39f12",
       "IPY_MODEL_e328628498bf435aa173df3486f5287e",
       "IPY_MODEL_0f527cf382624bf6b7c5fbd2ba773f1d"
      ],
      "layout": "IPY_MODEL_d83209d07507487d941f909b3c063786"
     }
    },
    "0279241d8728444eab9cabcd8fc39f12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7cdfb5d965d4d1fa845b0d6d7f34267",
      "placeholder": "​",
      "style": "IPY_MODEL_8ce7bb8afff74c7291f908fbabc4964d",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "e328628498bf435aa173df3486f5287e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dad9721c3fb410a803bb8edfe868f0b",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_194ceaf6c8dd4a96bcd1324edb59e4ce",
      "value": 350
     }
    },
    "0f527cf382624bf6b7c5fbd2ba773f1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e07c4f725834afa82a743a71566ec40",
      "placeholder": "​",
      "style": "IPY_MODEL_01e781df3070433baad362d7bb08b2e1",
      "value": " 350/350 [00:00&lt;00:00, 7.66kB/s]"
     }
    },
    "d83209d07507487d941f909b3c063786": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7cdfb5d965d4d1fa845b0d6d7f34267": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ce7bb8afff74c7291f908fbabc4964d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8dad9721c3fb410a803bb8edfe868f0b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "194ceaf6c8dd4a96bcd1324edb59e4ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e07c4f725834afa82a743a71566ec40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01e781df3070433baad362d7bb08b2e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08a1b4effd0b4aa7a9ec8175a97dc13e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6775e9ef4fa3479f8ec5bd038b88f75c",
       "IPY_MODEL_35e38d0fb8944aa298b006f32bda4b9d",
       "IPY_MODEL_64c44113eba8423a95c3a1bf8cf64167"
      ],
      "layout": "IPY_MODEL_13575f00bd9d48b9a7322b61f56c702d"
     }
    },
    "6775e9ef4fa3479f8ec5bd038b88f75c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca4fbfb851884131b1847c65c8778c89",
      "placeholder": "​",
      "style": "IPY_MODEL_f29b36b5a1ab4fc387d4f4478ecd393b",
      "value": "vocab.txt: 100%"
     }
    },
    "35e38d0fb8944aa298b006f32bda4b9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e18dc7cc356f49af96e57f93550acee9",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6050b32c7d9b4431ab8caccde40871bc",
      "value": 231508
     }
    },
    "64c44113eba8423a95c3a1bf8cf64167": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c77372d15cd54e6695eac372324056e2",
      "placeholder": "​",
      "style": "IPY_MODEL_01a7be094ebb499b8d43e1bdd47f1acb",
      "value": " 232k/232k [00:00&lt;00:00, 3.41MB/s]"
     }
    },
    "13575f00bd9d48b9a7322b61f56c702d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca4fbfb851884131b1847c65c8778c89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f29b36b5a1ab4fc387d4f4478ecd393b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e18dc7cc356f49af96e57f93550acee9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6050b32c7d9b4431ab8caccde40871bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c77372d15cd54e6695eac372324056e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01a7be094ebb499b8d43e1bdd47f1acb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98cfb6bc08fa4804b4e4e463fb77b38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4ec3a1b57da48d190ba613594947b45",
       "IPY_MODEL_9522edae5f79451dbcc421a07d5b81b3",
       "IPY_MODEL_8744fb699c79462788e35fdb261a1b46"
      ],
      "layout": "IPY_MODEL_5d8d93dd494447a496c41f49ca3d574a"
     }
    },
    "d4ec3a1b57da48d190ba613594947b45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c4671f1a1c949c0b1a37c17ec2ed398",
      "placeholder": "​",
      "style": "IPY_MODEL_a333f6f3447a400db47388e9e10ebd1e",
      "value": "tokenizer.json: 100%"
     }
    },
    "9522edae5f79451dbcc421a07d5b81b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8a35171bfa34da5b0ea95b47ddce1df",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae76274bfb404d5f8bf9252af89b2323",
      "value": 466247
     }
    },
    "8744fb699c79462788e35fdb261a1b46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f68ebe10449e4c3886373f05fe744081",
      "placeholder": "​",
      "style": "IPY_MODEL_d58fe0dcb67547a2a582ae4fc12c0ab9",
      "value": " 466k/466k [00:00&lt;00:00, 9.44MB/s]"
     }
    },
    "5d8d93dd494447a496c41f49ca3d574a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c4671f1a1c949c0b1a37c17ec2ed398": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a333f6f3447a400db47388e9e10ebd1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8a35171bfa34da5b0ea95b47ddce1df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae76274bfb404d5f8bf9252af89b2323": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f68ebe10449e4c3886373f05fe744081": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d58fe0dcb67547a2a582ae4fc12c0ab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcf7d1545b7440bca0c77b2e3379afb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be86bde53c9e4f3a9d677f8702196bc2",
       "IPY_MODEL_0ccc737c9ccb469b963b47d423b1c1a3",
       "IPY_MODEL_45861f8609644ef4bfb0913933729b21"
      ],
      "layout": "IPY_MODEL_6717201a04084dea97539e9961209384"
     }
    },
    "be86bde53c9e4f3a9d677f8702196bc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fca86c801b87418da6d750d50c83b1fd",
      "placeholder": "​",
      "style": "IPY_MODEL_f4228ef3961540e5bdbb5ba0a9fa9497",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "0ccc737c9ccb469b963b47d423b1c1a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcc99fe5dbf44770b5bc1be05e414638",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33f5a64520eb429f856b11918d78b901",
      "value": 112
     }
    },
    "45861f8609644ef4bfb0913933729b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c729ea16a654ccfaac820c47107c071",
      "placeholder": "​",
      "style": "IPY_MODEL_a9f4417f49654c958b39bc7d95052e4c",
      "value": " 112/112 [00:00&lt;00:00, 3.08kB/s]"
     }
    },
    "6717201a04084dea97539e9961209384": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fca86c801b87418da6d750d50c83b1fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4228ef3961540e5bdbb5ba0a9fa9497": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcc99fe5dbf44770b5bc1be05e414638": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33f5a64520eb429f856b11918d78b901": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c729ea16a654ccfaac820c47107c071": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9f4417f49654c958b39bc7d95052e4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2baf3ed32fe4f58ac2e60bdfd59a802": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a71eba2843954958b69bec2f4378572c",
       "IPY_MODEL_7af33cf79e364c6cb7d6a240919e7125",
       "IPY_MODEL_4fe5a07bd98b429fad97e446838eff39"
      ],
      "layout": "IPY_MODEL_46e346b7b5eb48158fd6d104dabc3b3e"
     }
    },
    "a71eba2843954958b69bec2f4378572c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0d072bfe9d548409aa415ee7679fe8a",
      "placeholder": "​",
      "style": "IPY_MODEL_46c63eb291ab4ba8a2e3b9196b346720",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "7af33cf79e364c6cb7d6a240919e7125": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_183a97da33464664b1c619f629f5c816",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75cc30e52092405fbd78b63ec501a88c",
      "value": 190
     }
    },
    "4fe5a07bd98b429fad97e446838eff39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d308ce7c059549268ee5b90ae7d9b02d",
      "placeholder": "​",
      "style": "IPY_MODEL_e2e702d303944f9dace4b5887ac0d229",
      "value": " 190/190 [00:00&lt;00:00, 3.19kB/s]"
     }
    },
    "46e346b7b5eb48158fd6d104dabc3b3e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0d072bfe9d548409aa415ee7679fe8a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46c63eb291ab4ba8a2e3b9196b346720": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "183a97da33464664b1c619f629f5c816": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75cc30e52092405fbd78b63ec501a88c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d308ce7c059549268ee5b90ae7d9b02d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2e702d303944f9dace4b5887ac0d229": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
